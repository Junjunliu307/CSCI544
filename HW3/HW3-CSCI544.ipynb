{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69180d8e-9b6e-48c3-aee9-bb9835f0cbc4",
   "metadata": {},
   "source": [
    "# Task 1: Vocabulary Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae643036-5781-44ff-8126-8562bf2811d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary generated successfully. Total vocabulary size: 16920, <unk> occurrences: 32537\n"
     ]
    }
   ],
   "source": [
    "def create_vocab(train_file, output_file, threshold=3):\n",
    "    \"\"\"\n",
    "    Create a vocabulary from the training data:\n",
    "    1. Count the occurrences of each word.\n",
    "    2. Replace low-frequency words (occurrences below the threshold) with <unk>.\n",
    "    3. The generated vocabulary format:\n",
    "       - The first line contains <unk> and its total occurrence count.\n",
    "       - Other words are sorted in descending order of frequency.\n",
    "       - Each line follows the format: word \\t index \\t count.\n",
    "    \"\"\"\n",
    "    word_counts = {}\n",
    "\n",
    "    # Read training data and count word frequencies\n",
    "    with open(train_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:  # Empty line indicates end of a sentence\n",
    "                continue\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) != 3:\n",
    "                continue  # Skip lines that do not match the expected format\n",
    "            _, word, _ = parts\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "\n",
    "    # Construct vocabulary, grouping low-frequency words as <unk>\n",
    "    unk_count = 0\n",
    "    vocab = {}\n",
    "    for word, count in word_counts.items():\n",
    "        if count < threshold:\n",
    "            unk_count += count\n",
    "        else:\n",
    "            vocab[word] = count\n",
    "\n",
    "    # Create final vocabulary list: first entry is <unk>, followed by words sorted by frequency\n",
    "    final_vocab = []\n",
    "    final_vocab.append((\"<unk>\", unk_count))\n",
    "    sorted_vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "    final_vocab.extend(sorted_vocab)\n",
    "\n",
    "    # Write to output file vocab.txt, each line in the format: word \\t index \\t count\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "        for idx, (word, count) in enumerate(final_vocab):\n",
    "            out_f.write(f\"{word}\\t{idx}\\t{count}\\n\")\n",
    "\n",
    "    print(f\"Vocabulary generated successfully. Total vocabulary size: {len(final_vocab)}, <unk> occurrences: {unk_count}\")\n",
    "\n",
    "train_file = \"./data/train\"  # Path to training data file\n",
    "output_file = \"vocab.txt\"    # Path to output vocabulary file\n",
    "threshold = 3                # Words with occurrences below this threshold are replaced with <unk>\n",
    "create_vocab(train_file, output_file, threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f459ca-b724-490e-a98b-38126daf796e",
   "metadata": {},
   "source": [
    "# Task 2: Model Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd521350-5a18-4a5e-b38b-7963e3ca958d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM model generated successfully. Transition parameters: 1351, Emission parameters: 50286\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def learn_hmm(train_file, output_file):\n",
    "    \"\"\"\n",
    "    Learn an HMM model from the training data, computing transition and emission probabilities,\n",
    "    and save the model as a JSON file.\n",
    "\n",
    "    Transition Probability: t(s′|s) = count(s→s′) / (sum of count(s→*))\n",
    "    Emission Probability: e(x|s) = count(s→x) / (count(s) for emission)\n",
    "    \"\"\"\n",
    "    # Dictionary to count emissions: key is (tag, word)\n",
    "    emission_counts = {}\n",
    "    # Dictionary to store total occurrences of each tag (for emission probability denominator)\n",
    "    tag_total_emission = {}\n",
    "\n",
    "    # Dictionary to count transitions: key is (prev_tag, curr_tag)\n",
    "    transition_counts = {}\n",
    "    # Dictionary to store total transitions from each tag (for transition probability denominator)\n",
    "    transition_total = {}\n",
    "\n",
    "    # Read data sentence by sentence (sentences are separated by empty lines)\n",
    "    with open(train_file, 'r', encoding='utf-8') as f:\n",
    "        sentence_tags = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                # Compute transitions for the current sentence\n",
    "                for i in range(len(sentence_tags) - 1):\n",
    "                    prev_tag = sentence_tags[i]\n",
    "                    curr_tag = sentence_tags[i + 1]\n",
    "                    transition_counts[(prev_tag, curr_tag)] = transition_counts.get((prev_tag, curr_tag), 0) + 1\n",
    "                    transition_total[prev_tag] = transition_total.get(prev_tag, 0) + 1\n",
    "                sentence_tags = []\n",
    "                continue\n",
    "\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) != 3:\n",
    "                continue  # Skip lines that do not match the expected format\n",
    "            _, word, tag = parts\n",
    "\n",
    "            # Update emission counts\n",
    "            emission_counts[(tag, word)] = emission_counts.get((tag, word), 0) + 1\n",
    "            tag_total_emission[tag] = tag_total_emission.get(tag, 0) + 1\n",
    "\n",
    "            # Store tag sequence for transition counting\n",
    "            sentence_tags.append(tag)\n",
    "\n",
    "        # Process the last sentence (if there is no empty line at the end of the file)\n",
    "        if sentence_tags:\n",
    "            for i in range(len(sentence_tags) - 1):\n",
    "                prev_tag = sentence_tags[i]\n",
    "                curr_tag = sentence_tags[i + 1]\n",
    "                transition_counts[(prev_tag, curr_tag)] = transition_counts.get((prev_tag, curr_tag), 0) + 1\n",
    "                transition_total[prev_tag] = transition_total.get(prev_tag, 0) + 1\n",
    "\n",
    "    # Compute transition probabilities\n",
    "    transition_prob = {}\n",
    "    for (prev_tag, curr_tag), count in transition_counts.items():\n",
    "        total = transition_total[prev_tag]\n",
    "        # Format the key as \"(prev_tag, curr_tag)\"\n",
    "        key = f\"({prev_tag}, {curr_tag})\"\n",
    "        transition_prob[key] = count / total\n",
    "\n",
    "    # Compute emission probabilities\n",
    "    emission_prob = {}\n",
    "    for (tag, word), count in emission_counts.items():\n",
    "        total = tag_total_emission[tag]\n",
    "        key = f\"({tag}, {word})\"\n",
    "        emission_prob[key] = count / total\n",
    "\n",
    "    # Construct the HMM model as a JSON object\n",
    "    model = {\n",
    "        \"transition\": transition_prob,\n",
    "        \"emission\": emission_prob\n",
    "    }\n",
    "\n",
    "    # Save the model to a JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "        json.dump(model, out_f, indent=4)\n",
    "\n",
    "    print(f\"HMM model generated successfully. Transition parameters: {len(transition_prob)}, Emission parameters: {len(emission_prob)}\")\n",
    "\n",
    "train_file = \"./data/train\"  # Path to the training data file\n",
    "output_file = \"hmm.json\"     # Path to the output model file\n",
    "learn_hmm(train_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea84f92-a62e-4d8a-8f5f-b7074b0a5593",
   "metadata": {},
   "source": [
    "# Define Evaluate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cbab1a3-dfb5-4675-acfe-93d7354e5802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def evaluate_predictions(gold_file, pred_file):\n",
    "    \"\"\"\n",
    "    Call eval.py to evaluate the prediction file against the gold standard.\n",
    "\n",
    "    Parameters:\n",
    "      gold_file (str): Path to the gold standard file.\n",
    "      pred_file (str): Path to the prediction file (viterbi.out or greedy.out).\n",
    "    \"\"\"\n",
    "    # Run the eval.py script with the given arguments\n",
    "    result = subprocess.run(\n",
    "        [\"python\", \"eval.py\", \"-g\", gold_file, \"-p\", pred_file],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    # Print the evaluation results\n",
    "    print(f\"Evaluation results for {pred_file}:\")\n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"Errors:\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8212460-7bc5-4e92-8e9a-bcef1be4fd4a",
   "metadata": {},
   "source": [
    "# Task 3: Greedy Decoding with HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99fc0b16-d77b-4d91-a89e-d125aeddcf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy decoding completed. Predictions saved to dev_greedy.out\n",
      "Evaluation results for dev_greedy.out:\n",
      "total: 131768, correct: 122838, accuracy: 93.22%\n",
      "\n",
      "Greedy decoding completed. Predictions saved to greedy.out\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def greedy_decode(test_file, output_file, model_file):\n",
    "    \"\"\"\n",
    "    Perform part-of-speech tagging on test data using the greedy decoding algorithm \n",
    "    and save the predictions to the output file.\n",
    "\n",
    "    Parameters:\n",
    "    - test_file: Path to the test data file (contains index and word, sentences separated by empty lines)\n",
    "    - output_file: Path to the output file, following the same format as training data \n",
    "                  (each line: \"index\\tword\\tpredicted_tag\")\n",
    "    - model_file: JSON file containing the HMM model (includes transition and emission probabilities)\n",
    "    \"\"\"\n",
    "    # Load the HMM model (transition and emission probabilities stored as string keys like \"(NN, dog)\")\n",
    "    with open(model_file, 'r', encoding='utf-8') as f:\n",
    "        model = json.load(f)\n",
    "    transition = model[\"transition\"]\n",
    "    emission = model[\"emission\"]\n",
    "\n",
    "    # Convert string keys to tuples for easier lookup\n",
    "    def parse_key(key):\n",
    "        # Key format: \"(tag, word)\", remove parentheses and split by \", \"\n",
    "        key = key.strip(\"()\")\n",
    "        parts = key.split(\", \")\n",
    "        return tuple(parts)\n",
    "    \n",
    "    transition_prob = {parse_key(key): prob for key, prob in transition.items()}\n",
    "    emission_prob = {parse_key(key): prob for key, prob in emission.items()}\n",
    "\n",
    "    # Extract possible tags from emission probabilities\n",
    "    possible_tags = list({tag for (tag, _) in emission_prob.keys()})\n",
    "\n",
    "    # Function to get emission probability\n",
    "    def get_emission(tag, word):\n",
    "        # Try (tag, word), otherwise use (tag, \"<unk>\")\n",
    "        if (tag, word) in emission_prob:\n",
    "            return emission_prob[(tag, word)]\n",
    "        elif (tag, \"<unk>\") in emission_prob:\n",
    "            return emission_prob[(tag, \"<unk>\")]\n",
    "        else:\n",
    "            return 1e-10  # Very small probability for unseen words\n",
    "\n",
    "    # Function to get transition probability\n",
    "    def get_transition(prev_tag, tag):\n",
    "        return transition_prob.get((prev_tag, tag), 1e-10)\n",
    "\n",
    "    # Read test data and perform greedy decoding\n",
    "    output_lines = []\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        sentence = []  # Store the current sentence as (index, word) tuples\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if line == \"\":\n",
    "                # End of current sentence, perform prediction\n",
    "                if sentence:\n",
    "                    predicted_tags = []\n",
    "                    prev_tag = None\n",
    "                    for idx, (index, word) in enumerate(sentence):\n",
    "                        best_tag = None\n",
    "                        best_score = -1\n",
    "                        for tag in possible_tags:\n",
    "                            if idx == 0:\n",
    "                                # First word: consider only emission probability\n",
    "                                score = get_emission(tag, word)\n",
    "                            else:\n",
    "                                score = get_transition(prev_tag, tag) * get_emission(tag, word)\n",
    "                            if score > best_score:\n",
    "                                best_score = score\n",
    "                                best_tag = tag\n",
    "                        predicted_tags.append(best_tag)\n",
    "                        prev_tag = best_tag\n",
    "                    # Write predictions to output list (same format as training data)\n",
    "                    for (index, word), tag in zip(sentence, predicted_tags):\n",
    "                        output_lines.append(f\"{index}\\t{word}\\t{tag}\")\n",
    "                    output_lines.append(\"\")  # Empty line separates sentences\n",
    "                    sentence = []\n",
    "            else:\n",
    "                # Test data format: each line contains index and word (ignore third column if present)\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) >= 2:\n",
    "                    index = parts[0]\n",
    "                    word = parts[1]\n",
    "                    sentence.append((index, word))\n",
    "\n",
    "        # Process the last sentence if the file does not end with an empty line\n",
    "        if sentence:\n",
    "            predicted_tags = []\n",
    "            prev_tag = None\n",
    "            for idx, (index, word) in enumerate(sentence):\n",
    "                best_tag = None\n",
    "                best_score = -1\n",
    "                for tag in possible_tags:\n",
    "                    if idx == 0:\n",
    "                        score = get_emission(tag, word)\n",
    "                    else:\n",
    "                        score = get_transition(prev_tag, tag) * get_emission(tag, word)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_tag = tag\n",
    "                predicted_tags.append(best_tag)\n",
    "                prev_tag = best_tag\n",
    "            for (index, word), tag in zip(sentence, predicted_tags):\n",
    "                output_lines.append(f\"{index}\\t{word}\\t{tag}\")\n",
    "            output_lines.append(\"\")\n",
    "\n",
    "    # Save predictions to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        f_out.write(\"\\n\".join(output_lines))\n",
    "\n",
    "    print(f\"Greedy decoding completed. Predictions saved to {output_file}\")\n",
    "\n",
    "# Update file paths as needed:\n",
    "test_file = \"./data/dev\"     # Test data file (each line: index \\t word, sentences separated by empty lines)\n",
    "output_file = \"dev_greedy.out\"    # Output file for predictions\n",
    "model_file = \"hmm.json\"       # Previously generated HMM model file\n",
    "greedy_decode(test_file, output_file, model_file)\n",
    "evaluate_predictions(\"./data/dev\", output_file)\n",
    "\n",
    "# Test File\n",
    "test_file = \"./data/test\"     # Test data file (each line: index \\t word, sentences separated by empty lines)\n",
    "output_file = \"greedy.out\"    # Output file for predictions\n",
    "model_file = \"hmm.json\"       # Previously generated HMM model file\n",
    "greedy_decode(test_file, output_file, model_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab6cbe7-24ec-4e34-8426-46a698682a70",
   "metadata": {},
   "source": [
    "# Task 4: Viterbi Decoding with HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bfb7ea2-cb1d-41da-80f8-f091a29f677d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi decoding completed. Predictions saved to dev_viterbi.out\n",
      "Evaluation results for dev_viterbi.out:\n",
      "total: 131768, correct: 124643, accuracy: 94.59%\n",
      "\n",
      "Viterbi decoding completed. Predictions saved to viterbi.out\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def viterbi_decode(test_file, output_file, model_file):\n",
    "    \"\"\"\n",
    "    Perform part-of-speech tagging on test data using the Viterbi decoding algorithm\n",
    "    and save the results to the output file.\n",
    "\n",
    "    Parameters:\n",
    "      - test_file: Path to the test data file (each line format: index \\t word, sentences separated by empty lines)\n",
    "      - output_file: Path to the output file, following the same format as the training data \n",
    "                     (\"index \\t word \\t predicted_tag\")\n",
    "      - model_file: JSON file containing the HMM model (transition and emission probabilities)\n",
    "    \"\"\"\n",
    "    # Load the HMM model\n",
    "    with open(model_file, 'r', encoding='utf-8') as f:\n",
    "        model = json.load(f)\n",
    "    transition = model[\"transition\"]\n",
    "    emission = model[\"emission\"]\n",
    "\n",
    "    # Convert string keys \"(tag, word)\" into tuples (tag, word) for easy lookup\n",
    "    def parse_key(key):\n",
    "        key = key.strip(\"()\")\n",
    "        parts = key.split(\", \")\n",
    "        return tuple(parts)\n",
    "\n",
    "    transition_prob = {parse_key(key): prob for key, prob in transition.items()}\n",
    "    emission_prob = {parse_key(key): prob for key, prob in emission.items()}\n",
    "\n",
    "    # Extract all possible tags from emission probabilities\n",
    "    possible_tags = list({tag for (tag, _) in emission_prob.keys()})\n",
    "\n",
    "    # Function to get emission probability\n",
    "    def get_emission(tag, word):\n",
    "        # Try (tag, word) first, if not found, try (tag, \"<unk>\"), otherwise return a very small probability\n",
    "        return emission_prob.get((tag, word), emission_prob.get((tag, \"<unk>\"), 1e-10))\n",
    "\n",
    "    # Function to get transition probability\n",
    "    def get_transition(prev_tag, tag):\n",
    "        return transition_prob.get((prev_tag, tag), 1e-10)\n",
    "\n",
    "    # Viterbi algorithm for decoding, input is a sentence (list of (index, word) tuples)\n",
    "    def viterbi(sentence):\n",
    "        n = len(sentence)\n",
    "        # dp[t][tag] stores the highest probability of reaching tag at time step t\n",
    "        dp = [{} for _ in range(n)]\n",
    "        # backpointer[t][tag] stores the best previous tag leading to the current tag\n",
    "        backpointer = [{} for _ in range(n)]\n",
    "        \n",
    "        # Initialization: First word only considers emission probability\n",
    "        first_word = sentence[0][1]\n",
    "        for tag in possible_tags:\n",
    "            dp[0][tag] = get_emission(tag, first_word)\n",
    "            backpointer[0][tag] = None\n",
    "        \n",
    "        # Dynamic programming step\n",
    "        for t in range(1, n):\n",
    "            word = sentence[t][1]\n",
    "            for curr_tag in possible_tags:\n",
    "                best_score = -1\n",
    "                best_prev = None\n",
    "                for prev_tag in possible_tags:\n",
    "                    score = dp[t-1][prev_tag] * get_transition(prev_tag, curr_tag) * get_emission(curr_tag, word)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_prev = prev_tag\n",
    "                dp[t][curr_tag] = best_score\n",
    "                backpointer[t][curr_tag] = best_prev\n",
    "\n",
    "        # Termination: Find the best tag at the last time step\n",
    "        best_last_tag = max(dp[n-1], key=dp[n-1].get)\n",
    "        \n",
    "        # Backtrack to get the best tag sequence\n",
    "        best_tags = [None] * n\n",
    "        best_tags[n-1] = best_last_tag\n",
    "        for t in range(n-1, 0, -1):\n",
    "            best_tags[t-1] = backpointer[t][best_tags[t]]\n",
    "        return best_tags\n",
    "\n",
    "    # Read test data, decode each sentence, and save the predictions\n",
    "    output_lines = []\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        sentence = []  # Store the current sentence as (index, word) tuples\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if line == \"\":\n",
    "                if sentence:\n",
    "                    predicted_tags = viterbi(sentence)\n",
    "                    for (index, word), tag in zip(sentence, predicted_tags):\n",
    "                        output_lines.append(f\"{index}\\t{word}\\t{tag}\")\n",
    "                    output_lines.append(\"\")  # Empty line separates sentences\n",
    "                    sentence = []\n",
    "            else:\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) >= 2:\n",
    "                    index = parts[0]\n",
    "                    word = parts[1]\n",
    "                    sentence.append((index, word))\n",
    "\n",
    "        # Process the last sentence if the file does not end with an empty line\n",
    "        if sentence:\n",
    "            predicted_tags = viterbi(sentence)\n",
    "            for (index, word), tag in zip(sentence, predicted_tags):\n",
    "                output_lines.append(f\"{index}\\t{word}\\t{tag}\")\n",
    "            output_lines.append(\"\")\n",
    "\n",
    "    # Write predictions to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "        out_f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "    print(f\"Viterbi decoding completed. Predictions saved to {output_file}\")\n",
    "\n",
    "# Update file paths as needed\n",
    "test_file = \"./data/dev\"     # Test data file, each line in the format: \"index \\t word\", sentences separated by empty lines\n",
    "output_file = \"dev_viterbi.out\"   # Output file for predictions\n",
    "model_file = \"hmm.json\"       # HMM model file\n",
    "viterbi_decode(test_file, output_file, model_file)\n",
    "evaluate_predictions(\"./data/dev\", output_file)\n",
    "\n",
    "# Test file\n",
    "test_file = \"./data/test\"     # Test data file, each line in the format: \"index \\t word\", sentences separated by empty lines\n",
    "output_file = \"viterbi.out\"   # Output file for predictions\n",
    "model_file = \"hmm.json\"       # HMM model file\n",
    "viterbi_decode(test_file, output_file, model_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
