{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33642cb3-1ef3-4f15-b113-320648c7eb0a",
   "metadata": {},
   "source": [
    "# Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "571460e9-2205-4597-8fb3-abcfe00228f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "# ----------------------------\n",
    "# Model Definition\n",
    "# ----------------------------\n",
    "class BLSTM_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, linear_dim, num_tags, dropout=0.33):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Vocabulary size.\n",
    "            embedding_dim: Embedding layer dimension (100).\n",
    "            hidden_dim: LSTM hidden layer dimension (256).\n",
    "            linear_dim: Linear layer output dimension (128).\n",
    "            num_tags: Number of NER tags.\n",
    "            dropout: Dropout rate for LSTM layer.\n",
    "        \"\"\"\n",
    "        super(BLSTM_NER, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1,\n",
    "                            bidirectional=True, dropout=dropout, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, linear_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.classifier = nn.Linear(linear_dim, num_tags)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        linear_out = self.linear(lstm_out)\n",
    "        elu_out = self.elu(linear_out)\n",
    "        logits = self.classifier(elu_out)\n",
    "        return logits\n",
    "\n",
    "# ----------------------------\n",
    "# Data Loading\n",
    "# ----------------------------\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, file_path, word_to_idx=None, tag_to_idx=None, build_vocab=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_path: Path to dataset (sentences separated by blank lines).\n",
    "            word_to_idx, tag_to_idx: Pre-built mappings if available, else use build_vocab.\n",
    "            build_vocab: Flag to build vocabulary and tag mappings from data.\n",
    "        \"\"\"\n",
    "        self.sentences = []\n",
    "        self.tags = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            sentence_words = []\n",
    "            sentence_tags = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line == \"\":\n",
    "                    if sentence_words:\n",
    "                        self.sentences.append(sentence_words)\n",
    "                        self.tags.append(sentence_tags)\n",
    "                        sentence_words = []\n",
    "                        sentence_tags = []\n",
    "                else:\n",
    "                    parts = line.split()\n",
    "                    if len(parts) >= 3:\n",
    "                        sentence_words.append(parts[1])\n",
    "                        sentence_tags.append(parts[2])\n",
    "            if sentence_words:\n",
    "                self.sentences.append(sentence_words)\n",
    "                self.tags.append(sentence_tags)\n",
    "\n",
    "        if build_vocab:\n",
    "            self.build_vocab()\n",
    "        else:\n",
    "            self.word_to_idx = word_to_idx\n",
    "            self.tag_to_idx = tag_to_idx\n",
    "\n",
    "    def build_vocab(self):\n",
    "        words = {word for sent in self.sentences for word in sent}\n",
    "        tags = {tag for tag_seq in self.tags for tag in tag_seq}\n",
    "        self.word_to_idx = {word: i + 2 for i, word in enumerate(sorted(words))}\n",
    "        self.word_to_idx[\"<PAD>\"] = 0\n",
    "        self.word_to_idx[\"<UNK>\"] = 1\n",
    "        self.tag_to_idx = {tag: i for i, tag in enumerate(sorted(tags))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word_indices = [self.word_to_idx.get(w, self.word_to_idx[\"<UNK>\"]) for w in self.sentences[idx]]\n",
    "        tag_indices = [self.tag_to_idx[t] for t in self.tags[idx]]\n",
    "        return torch.tensor(word_indices, dtype=torch.long), torch.tensor(tag_indices, dtype=torch.long)\n",
    "\n",
    "def pad_collate(batch):\n",
    "    words, tags = zip(*batch)\n",
    "    max_len = max(len(seq) for seq in words)\n",
    "    padded_words = [torch.cat([w, torch.zeros(max_len - len(w), dtype=torch.long)]) for w in words]\n",
    "    padded_tags = [torch.cat([t, torch.full((max_len - len(t),), -100, dtype=torch.long)]) for t in tags]\n",
    "    return torch.stack(padded_words), torch.stack(padded_tags), [len(seq) for seq in words]\n",
    "\n",
    "# ----------------------------\n",
    "# Training Function\n",
    "# ----------------------------\n",
    "def train_model(model, train_loader, optimizer, scheduler, criterion, device, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for words, tags, lengths in train_loader:\n",
    "            words, tags = words.to(device), tags.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(words).view(-1, model.classifier.out_features)\n",
    "            loss = criterion(outputs, tags.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        scheduler.step(epoch_loss / len(train_loader))\n",
    "        print(f\"Epoch {epoch+1} Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Generate Test Predictions\n",
    "# ----------------------------\n",
    "class NERTestDataset(Dataset):\n",
    "    def __init__(self, file_path, word_to_idx):\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.sentences = []\n",
    "        self.raw_lines = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            sentence_words, sentence_lines = [], []\n",
    "            for line in f:\n",
    "                if line.strip() == \"\":\n",
    "                    if sentence_words:\n",
    "                        self.sentences.append(sentence_words)\n",
    "                        self.raw_lines.append(sentence_lines)\n",
    "                        sentence_words, sentence_lines = [], []\n",
    "                else:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 2:\n",
    "                        sentence_words.append(parts[1])\n",
    "                        sentence_lines.append((parts[0], parts[1]))\n",
    "            if sentence_words:\n",
    "                self.sentences.append(sentence_words)\n",
    "                self.raw_lines.append(sentence_lines)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor([self.word_to_idx.get(w, self.word_to_idx[\"<UNK>\"]) for w in self.sentences[idx]], dtype=torch.long), self.raw_lines[idx]\n",
    "\n",
    "def pad_collate_test(batch):\n",
    "    words, raw_lines = zip(*batch)\n",
    "    max_len = max(len(seq) for seq in words)\n",
    "    padded_words = [torch.cat([w, torch.zeros(max_len - len(w), dtype=torch.long)]) for w in words]\n",
    "    return torch.stack(padded_words), [len(seq) for seq in words], raw_lines\n",
    "\n",
    "def generate_test_predictions(model, test_dataset, device, output_file):\n",
    "    model.eval()\n",
    "    idx_to_tag = {0: \"O\", 1: \"B-PER\", 2: \"I-PER\", 3: \"B-LOC\", 4: \"I-LOC\", 5: \"B-ORG\", 6: \"I-ORG\"}\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=pad_collate_test)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as outf:\n",
    "        with torch.no_grad():\n",
    "            for words, lengths, raw_lines in test_loader:\n",
    "                predictions = torch.argmax(model(words.to(device)), dim=-1).squeeze(0)[:lengths[0]].cpu().tolist()\n",
    "                for ((idx, word), pred_idx) in zip(raw_lines[0], predictions):\n",
    "                    outf.write(f\"{idx} {word} {idx_to_tag.get(pred_idx, 'O')}\\n\")\n",
    "                outf.write(\"\\n\")\n",
    "    print(f\"Predictions saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "45a434c1-860d-41ea-bf56-3cad6734afa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.6756\n",
      "Epoch 2 Loss: 0.4712\n",
      "Epoch 3 Loss: 0.3639\n",
      "Epoch 4 Loss: 0.2842\n",
      "Epoch 5 Loss: 0.2217\n",
      "Epoch 6 Loss: 0.1714\n",
      "Epoch 7 Loss: 0.1320\n",
      "Epoch 8 Loss: 0.1001\n",
      "Epoch 9 Loss: 0.0770\n",
      "Epoch 10 Loss: 0.0570\n",
      "Calling evaluation script...\n",
      "processed 51578 tokens with 5942 phrases; found: 5923 phrases; correct: 4045.\n",
      "accuracy:  93.90%; precision:  68.29%; recall:  68.07%; FB1:  68.18\n",
      "              LOC: precision:  84.96%; recall:  71.64%; FB1:  77.73  1549\n",
      "             MISC: precision:  70.97%; recall:  69.20%; FB1:  70.07  899\n",
      "              ORG: precision:  62.51%; recall:  61.67%; FB1:  62.09  1323\n",
      "              PER: precision:  58.74%; recall:  68.62%; FB1:  63.29  2152\n",
      "Predictions saved to test1.out\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Main function\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load training dataset and build vocabulary/tag mapping\n",
    "train_dataset = NERDataset(\"data/train\", build_vocab=True)\n",
    "# Load development dataset using the same vocabulary/tag mapping as training data\n",
    "dev_dataset = NERDataset(\"data/dev\", word_to_idx=train_dataset.word_to_idx, tag_to_idx=train_dataset.tag_to_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=pad_collate)\n",
    "\n",
    "vocab_size = len(train_dataset.word_to_idx)\n",
    "num_tags = len(train_dataset.tag_to_idx)\n",
    "\n",
    "model = BLSTM_NER(vocab_size=vocab_size,\n",
    "                  embedding_dim=100,\n",
    "                  hidden_dim=256,\n",
    "                  linear_dim=128,\n",
    "                  num_tags=num_tags,\n",
    "                  dropout=0.33)\n",
    "model.to(device)\n",
    "\n",
    "# Use CrossEntropyLoss and ignore padded labels (-100)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Use SGD with momentum optimization\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Use ReduceLROnPlateau scheduler for adaptive learning rate\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "num_epochs = 10\n",
    "train_model(model, train_loader, optimizer, scheduler, criterion, device, num_epochs)\n",
    "torch.save(model.state_dict(), \"blstm1.pt\")\n",
    "\n",
    "# Generate prediction file for evaluation\n",
    "dev_pred_file = \"dev1.out\"\n",
    "generate_prediction_file(model, dev_dataset, device, dev_pred_file)\n",
    "\n",
    "# Call the provided eval.py script for evaluation (gold file is data/dev)\n",
    "cmd = f\"python eval.py -p {dev_pred_file} -g data/dev\"\n",
    "print(\"Calling evaluation script...\")\n",
    "os.system(cmd)\n",
    "\n",
    "test_dataset = NERTestDataset(\"data/test\", train_dataset.word_to_idx)\n",
    "generate_test_predictions(model, test_dataset, device, output_file=\"test1.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4cf6d4a4-793c-40f1-81f1-1363a3bf5a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting additional 10 epochs training...\n",
      "Epoch 1 Loss: 0.0407\n",
      "Epoch 2 Loss: 0.0276\n",
      "Epoch 3 Loss: 0.0183\n",
      "Epoch 4 Loss: 0.0121\n",
      "Epoch 5 Loss: 0.0084\n",
      "Epoch 6 Loss: 0.0060\n",
      "Epoch 7 Loss: 0.0044\n",
      "Epoch 8 Loss: 0.0045\n",
      "Epoch 9 Loss: 0.0034\n",
      "Epoch 10 Loss: 0.0025\n",
      "Calling evaluation script...\n",
      "processed 51578 tokens with 5942 phrases; found: 5770 phrases; correct: 4179.\n",
      "accuracy:  94.53%; precision:  72.43%; recall:  70.33%; FB1:  71.36\n",
      "              LOC: precision:  90.49%; recall:  74.09%; FB1:  81.47  1504\n",
      "             MISC: precision:  73.39%; recall:  74.19%; FB1:  73.79  932\n",
      "              ORG: precision:  70.56%; recall:  62.57%; FB1:  66.32  1189\n",
      "              PER: precision:  60.37%; recall:  70.30%; FB1:  64.96  2145\n",
      "Predictions saved to test1.out\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"blstm1.pt\"))\n",
    "additional_epochs = 10\n",
    "\n",
    "print(f\"Starting additional {additional_epochs} epochs training...\")\n",
    "train_model(model, train_loader, optimizer, scheduler, criterion, device, num_epochs=additional_epochs)\n",
    "torch.save(model.state_dict(), \"blstm1.pt\")\n",
    "\n",
    "# Generate prediction file for evaluation\n",
    "generate_prediction_file(model, dev_dataset, device, dev_pred_file)\n",
    "\n",
    "# Call the provided eval.py script for evaluation (gold file is data/dev)\n",
    "cmd = f\"python eval.py -p {dev_pred_file} -g data/dev\"\n",
    "print(\"Calling evaluation script...\")\n",
    "os.system(cmd)\n",
    "\n",
    "generate_test_predictions(model, test_dataset, device, output_file=\"test1.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b751ec03-1c00-41de-a9ab-13004aca1d42",
   "metadata": {},
   "source": [
    "# Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cb76438f-d716-4846-8d0c-345ecb8c2757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.4470\n",
      "Epoch 2 Loss: 0.2449\n",
      "Epoch 3 Loss: 0.1815\n",
      "Epoch 4 Loss: 0.1502\n",
      "Epoch 5 Loss: 0.1302\n",
      "Epoch 6 Loss: 0.1129\n",
      "Epoch 7 Loss: 0.0973\n",
      "Epoch 8 Loss: 0.0888\n",
      "Epoch 9 Loss: 0.0794\n",
      "Epoch 10 Loss: 0.0715\n",
      "Calling evaluation script...\n",
      "processed 51578 tokens with 5942 phrases; found: 5144 phrases; correct: 4374.\n",
      "accuracy:  95.57%; precision:  85.03%; recall:  73.61%; FB1:  78.91\n",
      "              LOC: precision:  92.06%; recall:  82.09%; FB1:  86.79  1638\n",
      "             MISC: precision:  84.05%; recall:  72.02%; FB1:  77.57  790\n",
      "              ORG: precision:  74.94%; recall:  69.13%; FB1:  71.92  1237\n",
      "              PER: precision:  86.21%; recall:  69.22%; FB1:  76.78  1479\n",
      "Predictions saved to test2.out\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Function to load GloVe embeddings\n",
    "# ----------------------------\n",
    "def load_glove_embeddings(glove_path, word_to_idx, embedding_dim=100):\n",
    "    \"\"\"\n",
    "    Loads GloVe embeddings and creates an embedding matrix.\n",
    "    For each word in the vocabulary, its lower-case version is used to look up in GloVe.\n",
    "    Words not found in GloVe are randomly initialized.\n",
    "    \"\"\"\n",
    "    embeddings = np.random.randn(len(word_to_idx), embedding_dim).astype(np.float32)\n",
    "    glove_dict = {}\n",
    "    with gzip.open(glove_path, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split()\n",
    "            if len(tokens) == embedding_dim + 1:\n",
    "                word = tokens[0]\n",
    "                vector = np.array(tokens[1:], dtype=np.float32)\n",
    "                glove_dict[word] = vector\n",
    "    # Initialize embeddings: use lower-case lookup for each word in our vocabulary.\n",
    "    for word, idx in word_to_idx.items():\n",
    "        glove_vector = glove_dict.get(word.lower())\n",
    "        if glove_vector is not None:\n",
    "            embeddings[idx] = glove_vector\n",
    "    return torch.tensor(embeddings)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load training dataset and build vocabulary/tag mapping.\n",
    "train_dataset = NERDataset(\"data/train\", build_vocab=True)\n",
    "# Load development dataset using the same vocabulary/tag mapping as training data.\n",
    "dev_dataset = NERDataset(\"data/dev\", word_to_idx=train_dataset.word_to_idx, tag_to_idx=train_dataset.tag_to_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=pad_collate)\n",
    "\n",
    "vocab_size = len(train_dataset.word_to_idx)\n",
    "num_tags = len(train_dataset.tag_to_idx)\n",
    "\n",
    "model = BLSTM_NER(vocab_size=vocab_size,\n",
    "                  embedding_dim=100,\n",
    "                  hidden_dim=256,\n",
    "                  linear_dim=128,\n",
    "                  num_tags=num_tags,\n",
    "                  dropout=0.33)\n",
    "model.to(device)\n",
    "\n",
    "# Load GloVe embeddings and initialize the embedding layer.\n",
    "glove_path = \"glove.6B.100d.gz\"\n",
    "glove_weights = load_glove_embeddings(glove_path, train_dataset.word_to_idx, embedding_dim=100)\n",
    "model.embedding.weight.data.copy_(glove_weights)\n",
    "\n",
    "# Use CrossEntropyLoss and ignore padded labels (-100).\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Use SGD with momentum.\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Use ReduceLROnPlateau scheduler for adaptive learning rate.\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "num_epochs = 10\n",
    "train_model(model, train_loader, optimizer, scheduler, criterion, device, num_epochs)\n",
    "torch.save(model.state_dict(), \"blstm2.pt\")\n",
    "\n",
    "# Generate prediction file for evaluation\n",
    "dev_pred_file = \"dev2.out\"\n",
    "generate_prediction_file(model, dev_dataset, device, dev_pred_file)\n",
    "\n",
    "# Call the provided eval.py script for evaluation (gold file is data/dev)\n",
    "cmd = f\"python eval.py -p {dev_pred_file} -g data/dev\"\n",
    "print(\"Calling evaluation script...\")\n",
    "os.system(cmd)\n",
    "\n",
    "test_dataset = NERTestDataset(\"data/test\", train_dataset.word_to_idx)\n",
    "generate_test_predictions(model, test_dataset, device, output_file=\"test2.out\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a66b20a7-26b3-47e4-8760-f8f87a61adc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting additional 5 epochs training...\n",
      "Epoch 1 Loss: 0.0660\n",
      "Epoch 2 Loss: 0.0590\n",
      "Epoch 3 Loss: 0.0541\n",
      "Epoch 4 Loss: 0.0492\n",
      "Epoch 5 Loss: 0.0432\n",
      "Calling evaluation script...\n",
      "processed 51578 tokens with 5942 phrases; found: 5391 phrases; correct: 4591.\n",
      "accuracy:  96.07%; precision:  85.16%; recall:  77.26%; FB1:  81.02\n",
      "              LOC: precision:  92.42%; recall:  84.98%; FB1:  88.54  1689\n",
      "             MISC: precision:  86.32%; recall:  75.27%; FB1:  80.42  804\n",
      "              ORG: precision:  78.48%; recall:  69.35%; FB1:  73.63  1185\n",
      "              PER: precision:  82.08%; recall:  76.33%; FB1:  79.10  1713\n",
      "Predictions saved to test2.out\n"
     ]
    }
   ],
   "source": [
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "model.load_state_dict(torch.load(\"blstm2.pt\"))\n",
    "additional_epochs = 5\n",
    "\n",
    "print(f\"Starting additional {additional_epochs} epochs training...\")\n",
    "train_model(model, train_loader, optimizer, scheduler, criterion, device, num_epochs=additional_epochs)\n",
    "torch.save(model.state_dict(), \"blstm2.pt\")\n",
    "\n",
    "# Generate prediction file for evaluation\n",
    "generate_prediction_file(model, dev_dataset, device, dev_pred_file)\n",
    "\n",
    "# Call the provided eval.py script for evaluation (gold file is data/dev)\n",
    "cmd = f\"python eval.py -p {dev_pred_file} -g data/dev\"\n",
    "print(\"Calling evaluation script...\")\n",
    "os.system(cmd)\n",
    "\n",
    "generate_test_predictions(model, test_dataset, device, output_file=\"test2.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfbd81b-f13e-4ee1-a256-a0de23ed2668",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bcdee2b0-6f5d-4789-bc59-9a94c2eda534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.3586\n",
      "Epoch 2 Loss: 0.1287\n",
      "Epoch 3 Loss: 0.0910\n",
      "Epoch 4 Loss: 0.0761\n",
      "Epoch 5 Loss: 0.0673\n",
      "Epoch 6 Loss: 0.0586\n",
      "Epoch 7 Loss: 0.0531\n",
      "Epoch 8 Loss: 0.0468\n",
      "Epoch 9 Loss: 0.0426\n",
      "Epoch 10 Loss: 0.0381\n",
      "Calling evaluation script on dev data...\n",
      "processed 51578 tokens with 5942 phrases; found: 6219 phrases; correct: 4909.\n",
      "accuracy:  96.92%; precision:  78.94%; recall:  82.62%; FB1:  80.73\n",
      "              LOC: precision:  94.02%; recall:  79.53%; FB1:  86.17  1554\n",
      "             MISC: precision:  78.03%; recall:  78.96%; FB1:  78.49  933\n",
      "              ORG: precision:  77.73%; recall:  71.07%; FB1:  74.25  1226\n",
      "              PER: precision:  70.51%; recall:  95.93%; FB1:  81.28  2506\n",
      "Test predictions saved to file 'pred'.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Dataset with Character-level Information\n",
    "# ----------------------------\n",
    "class NERDatasetWithChar(Dataset):\n",
    "    def __init__(self, file_path, word_to_idx=None, tag_to_idx=None, char_to_idx=None, build_vocab=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_path: Path to the data file.\n",
    "                      For training/dev: each non-blank line has \"index word tag\".\n",
    "                      For test: each non-blank line has \"index word\".\n",
    "                      Sentences are separated by blank lines.\n",
    "            word_to_idx: Pre-built word vocabulary mapping (if available).\n",
    "            tag_to_idx: Pre-built tag mapping (if available; not used for test).\n",
    "            char_to_idx: Pre-built character vocabulary mapping (if available).\n",
    "            build_vocab: Whether to build vocabularies from this file (usually on training data).\n",
    "        \"\"\"\n",
    "        self.sentences = []  # list of list of words\n",
    "        self.tags = []       # list of list of tags; for test files, this will be empty lists\n",
    "        self.indices = []    # list of list of (index, word) tuples (to preserve original order)\n",
    "        self.has_tags = True\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            sent_words = []\n",
    "            sent_tags = []\n",
    "            sent_idx_word = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line == \"\":\n",
    "                    if sent_words:\n",
    "                        self.sentences.append(sent_words)\n",
    "                        self.tags.append(sent_tags)\n",
    "                        self.indices.append(sent_idx_word)\n",
    "                        sent_words = []\n",
    "                        sent_tags = []\n",
    "                        sent_idx_word = []\n",
    "                    continue\n",
    "                tokens = line.split()\n",
    "                # Determine file type by number of tokens\n",
    "                if len(tokens) == 3:\n",
    "                    # training/dev file: index word tag\n",
    "                    idx, word, tag = tokens\n",
    "                    sent_words.append(word)\n",
    "                    sent_tags.append(tag)\n",
    "                    sent_idx_word.append((idx, word))\n",
    "                elif len(tokens) == 2:\n",
    "                    # test file: index word (no tag)\n",
    "                    idx, word = tokens\n",
    "                    sent_words.append(word)\n",
    "                    sent_tags.append(None)\n",
    "                    sent_idx_word.append((idx, word))\n",
    "                    self.has_tags = False\n",
    "                else:\n",
    "                    continue\n",
    "            if sent_words:\n",
    "                self.sentences.append(sent_words)\n",
    "                self.tags.append(sent_tags)\n",
    "                self.indices.append(sent_idx_word)\n",
    "\n",
    "        # If building vocabularies, do it now.\n",
    "        if build_vocab:\n",
    "            self.build_vocab()\n",
    "        else:\n",
    "            self.word_to_idx = word_to_idx\n",
    "            self.tag_to_idx = tag_to_idx\n",
    "            self.char_to_idx = char_to_idx\n",
    "\n",
    "    def build_vocab(self):\n",
    "        # Build word vocabulary (case-sensitive)\n",
    "        words = set()\n",
    "        tags = set()\n",
    "        chars = set()\n",
    "        for sent, tag_seq in zip(self.sentences, self.tags):\n",
    "            for word in sent:\n",
    "                words.add(word)\n",
    "                for ch in word:\n",
    "                    chars.add(ch)\n",
    "            # Only add tags if available (for training/dev)\n",
    "            if self.has_tags:\n",
    "                for tag in tag_seq:\n",
    "                    tags.add(tag)\n",
    "        # Reserve indices: 0 for <PAD>, 1 for <UNK>\n",
    "        self.word_to_idx = {word: i+2 for i, word in enumerate(sorted(words))}\n",
    "        self.word_to_idx[\"<PAD>\"] = 0\n",
    "        self.word_to_idx[\"<UNK>\"] = 1\n",
    "        if self.has_tags:\n",
    "            self.tag_to_idx = {tag: i for i, tag in enumerate(sorted(tags))}\n",
    "        else:\n",
    "            self.tag_to_idx = None\n",
    "        # Build character vocabulary similarly (0 for <PAD>, 1 for <UNK>)\n",
    "        self.char_to_idx = {ch: i+2 for i, ch in enumerate(sorted(chars))}\n",
    "        self.char_to_idx[\"<PAD>\"] = 0\n",
    "        self.char_to_idx[\"<UNK>\"] = 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert words to indices and also produce character indices for each word.\n",
    "        words = self.sentences[idx]\n",
    "        word_indices = [self.word_to_idx.get(w, self.word_to_idx[\"<UNK>\"]) for w in words]\n",
    "        # For each word, convert characters to indices\n",
    "        char_indices = []\n",
    "        for word in words:\n",
    "            ch_idx = [self.char_to_idx.get(ch, self.char_to_idx[\"<UNK>\"]) for ch in word]\n",
    "            char_indices.append(ch_idx)\n",
    "        if self.has_tags:\n",
    "            tag_seq = self.tags[idx]\n",
    "            tag_indices = [self.tag_to_idx[t] for t in tag_seq]\n",
    "        else:\n",
    "            tag_indices = None\n",
    "        return torch.tensor(word_indices, dtype=torch.long), \\\n",
    "               (torch.tensor(tag_indices, dtype=torch.long) if tag_indices is not None else None), \\\n",
    "               char_indices\n",
    "\n",
    "# ----------------------------\n",
    "# Collate function for batching with character sequences\n",
    "# ----------------------------\n",
    "def pad_collate_with_char(batch):\n",
    "    \"\"\"\n",
    "    Pads word sequences, tag sequences, and character sequences.\n",
    "    Returns:\n",
    "      padded_words: LongTensor of shape (batch_size, max_seq_len)\n",
    "      padded_tags: LongTensor of shape (batch_size, max_seq_len) or None if tags not available\n",
    "      lengths: list of original sentence lengths\n",
    "      padded_chars: LongTensor of shape (batch_size, max_seq_len, max_word_len)\n",
    "    \"\"\"\n",
    "    batch_size = len(batch)\n",
    "    word_seqs = [item[0] for item in batch]  # each is a tensor of word indices\n",
    "    tag_seqs = [item[1] for item in batch]     # each is a tensor of tag indices or None\n",
    "    char_seqs = [item[2] for item in batch]      # list of list of lists\n",
    "\n",
    "    lengths = [len(seq) for seq in word_seqs]\n",
    "    max_seq_len = max(lengths)\n",
    "    \n",
    "    # Pad word sequences\n",
    "    padded_words = []\n",
    "    for seq in word_seqs:\n",
    "        pad_size = max_seq_len - seq.size(0)\n",
    "        if pad_size > 0:\n",
    "            padded_seq = torch.cat([seq, torch.zeros(pad_size, dtype=torch.long)])\n",
    "        else:\n",
    "            padded_seq = seq\n",
    "        padded_words.append(padded_seq)\n",
    "    padded_words = torch.stack(padded_words)\n",
    "    \n",
    "    # Pad tag sequences if available\n",
    "    if tag_seqs[0] is not None:\n",
    "        padded_tags = []\n",
    "        for seq in tag_seqs:\n",
    "            pad_size = max_seq_len - seq.size(0)\n",
    "            if pad_size > 0:\n",
    "                padded_seq = torch.cat([seq, torch.full((pad_size,), -100, dtype=torch.long)])\n",
    "            else:\n",
    "                padded_seq = seq\n",
    "            padded_tags.append(padded_seq)\n",
    "        padded_tags = torch.stack(padded_tags)\n",
    "    else:\n",
    "        padded_tags = None\n",
    "\n",
    "    # For character sequences: first determine maximum word length in the batch\n",
    "    max_word_len = 0\n",
    "    for sent in char_seqs:\n",
    "        for word in sent:\n",
    "            if len(word) > max_word_len:\n",
    "                max_word_len = len(word)\n",
    "    # Pad each word's character list, and also pad sentences with fewer words.\n",
    "    padded_chars = []\n",
    "    for sent in char_seqs:\n",
    "        # Pad each word in the sentence\n",
    "        padded_sent = []\n",
    "        for word in sent:\n",
    "            pad_len = max_word_len - len(word)\n",
    "            padded_word = word + [0]*pad_len\n",
    "            padded_sent.append(padded_word)\n",
    "        # If sentence length < max_seq_len, add padding for missing words.\n",
    "        for _ in range(max_seq_len - len(sent)):\n",
    "            padded_sent.append([0]*max_word_len)\n",
    "        padded_chars.append(padded_sent)\n",
    "    padded_chars = torch.tensor(padded_chars, dtype=torch.long)  # shape: (batch_size, max_seq_len, max_word_len)\n",
    "    \n",
    "    return padded_words, padded_tags, lengths, padded_chars\n",
    "\n",
    "# ----------------------------\n",
    "# LSTM-CNN NER Model with Character-level CNN\n",
    "# ----------------------------\n",
    "class LSTM_CNN_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, char_vocab_size, embedding_dim, char_embedding_dim,\n",
    "                 char_cnn_out_dim, hidden_dim, linear_dim, num_tags, dropout=0.33):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Size of the word vocabulary.\n",
    "            char_vocab_size: Size of the character vocabulary.\n",
    "            embedding_dim: Dimension of the word embeddings (e.g., 100).\n",
    "            char_embedding_dim: Dimension of the character embeddings (set to 30).\n",
    "            char_cnn_out_dim: Output dimension of the character-level CNN (e.g., 50).\n",
    "            hidden_dim: Hidden dimension of the BLSTM (256).\n",
    "            linear_dim: Dimension of the intermediate Linear layer (128).\n",
    "            num_tags: Number of NER tags.\n",
    "            dropout: Dropout rate for the BLSTM.\n",
    "        \"\"\"\n",
    "        super(LSTM_CNN_NER, self).__init__()\n",
    "        # Word embedding layer (will be initialized with GloVe later)\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Character embedding layer (randomly initialized, padding index=0)\n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim, padding_idx=0)\n",
    "        # CNN module for characters: use 1D convolution with kernel size 3 and padding=1\n",
    "        self.char_cnn = nn.Conv1d(in_channels=char_embedding_dim, out_channels=char_cnn_out_dim, kernel_size=3, padding=1)\n",
    "        # BLSTM layer: input dimension is word_embedding + char_cnn output (100+char_cnn_out_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim + char_cnn_out_dim, hidden_dim, num_layers=1,\n",
    "                            bidirectional=True, dropout=dropout, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, linear_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.classifier = nn.Linear(linear_dim, num_tags)\n",
    "\n",
    "    def forward(self, word_inputs, char_inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word_inputs: LongTensor of shape (batch_size, seq_len) containing word indices.\n",
    "            char_inputs: LongTensor of shape (batch_size, seq_len, max_word_len) containing character indices.\n",
    "        Returns:\n",
    "            logits: Tensor of shape (batch_size, seq_len, num_tags)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = word_inputs.size()\n",
    "        # Word embeddings: (batch_size, seq_len, embedding_dim)\n",
    "        word_embeds = self.word_embedding(word_inputs)\n",
    "        \n",
    "        # Process character-level inputs\n",
    "        # char_inputs shape: (batch_size, seq_len, max_word_len)\n",
    "        # Reshape to (batch_size*seq_len, max_word_len)\n",
    "        char_inputs = char_inputs.view(-1, char_inputs.size(2))\n",
    "        # Get char embeddings: (batch_size*seq_len, max_word_len, char_embedding_dim)\n",
    "        char_embeds = self.char_embedding(char_inputs)\n",
    "        # Permute to (batch_size*seq_len, char_embedding_dim, max_word_len) for CNN\n",
    "        char_embeds = char_embeds.permute(0, 2, 1)\n",
    "        # Apply CNN: output shape -> (batch_size*seq_len, char_cnn_out_dim, max_word_len)\n",
    "        char_cnn_out = self.char_cnn(char_embeds)\n",
    "        # Apply ReLU\n",
    "        char_cnn_out = torch.relu(char_cnn_out)\n",
    "        # Apply max pooling over time dimension (kernel = entire sequence length)\n",
    "        char_rep, _ = torch.max(char_cnn_out, dim=2)  # shape: (batch_size*seq_len, char_cnn_out_dim)\n",
    "        # Reshape back to (batch_size, seq_len, char_cnn_out_dim)\n",
    "        char_rep = char_rep.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Concatenate word embeddings and character-level representations: (batch_size, seq_len, embedding_dim + char_cnn_out_dim)\n",
    "        combined = torch.cat([word_embeds, char_rep], dim=2)\n",
    "        \n",
    "        # BLSTM layer\n",
    "        lstm_out, _ = self.lstm(combined)  # shape: (batch_size, seq_len, hidden_dim*2)\n",
    "        # Linear, ELU, and classifier layers\n",
    "        linear_out = self.linear(lstm_out)  # shape: (batch_size, seq_len, linear_dim)\n",
    "        elu_out = self.elu(linear_out)\n",
    "        logits = self.classifier(elu_out)   # shape: (batch_size, seq_len, num_tags)\n",
    "        return logits\n",
    "\n",
    "# ----------------------------\n",
    "# Training function\n",
    "# ----------------------------\n",
    "def train_model(model, train_loader, optimizer, scheduler, criterion, device, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for word_inputs, tag_inputs, lengths, char_inputs in train_loader:\n",
    "            word_inputs = word_inputs.to(device)\n",
    "            char_inputs = char_inputs.to(device)\n",
    "            tag_inputs = tag_inputs.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(word_inputs, char_inputs)  # (batch_size, seq_len, num_tags)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "            tag_inputs = tag_inputs.view(-1)\n",
    "            loss = criterion(outputs, tag_inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        # Update scheduler based on the average epoch loss\n",
    "        scheduler.step(avg_loss)\n",
    "        print(f\"Epoch {epoch+1} Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Generate prediction file (for dev/test) using eval.py\n",
    "# ----------------------------\n",
    "def generate_prediction_file(model, dataset, device, pred_file_path):\n",
    "    \"\"\"\n",
    "    Generates a prediction file.\n",
    "    The output format per line is: index word predicted_tag\n",
    "    Blank lines separate sentences.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Build mapping from index to tag\n",
    "    idx_to_tag = {v: k for k, v in dataset.tag_to_idx.items()} if dataset.has_tags else None\n",
    "    # To store predicted tag sequences for each sentence\n",
    "    predicted_sentences = []\n",
    "    \n",
    "    # Use DataLoader with batch_size=1 to preserve order\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=pad_collate_with_char)\n",
    "    with torch.no_grad():\n",
    "        for word_inputs, tag_inputs, lengths, char_inputs in loader:\n",
    "            word_inputs = word_inputs.to(device)\n",
    "            char_inputs = char_inputs.to(device)\n",
    "            outputs = model(word_inputs, char_inputs)  # (1, seq_len, num_tags)\n",
    "            predictions = torch.argmax(outputs, dim=-1).squeeze(0)  # (seq_len,)\n",
    "            seq_len = lengths[0]\n",
    "            pred_seq = predictions[:seq_len].cpu().tolist()\n",
    "            if idx_to_tag is not None:\n",
    "                pred_tags = [idx_to_tag[idx] for idx in pred_seq]\n",
    "            else:\n",
    "                # For test data, if tag mapping not available, output a dummy tag (e.g., \"O\")\n",
    "                pred_tags = [\"O\"] * seq_len\n",
    "            predicted_sentences.append(pred_tags)\n",
    "    \n",
    "    # Write predictions in the original file format (using dataset.indices for index and word)\n",
    "    with open(pred_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sent_pred, sent_raw in zip(predicted_sentences, dataset.indices):\n",
    "            for (idx, word), tag in zip(sent_raw, sent_pred):\n",
    "                f.write(f\"{idx} {word} {tag}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "# ----------------------------\n",
    "# Main function\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----------------------------\n",
    "# Load training data and build vocabularies\n",
    "# ----------------------------\n",
    "train_file = \"data/train\"\n",
    "dev_file = \"data/dev\"\n",
    "test_file = \"data/test\"\n",
    "\n",
    "train_dataset = NERDatasetWithChar(train_file, build_vocab=True)\n",
    "# Use the same vocabularies for dev and test\n",
    "dev_dataset = NERDatasetWithChar(dev_file, word_to_idx=train_dataset.word_to_idx,\n",
    "                                  tag_to_idx=train_dataset.tag_to_idx,\n",
    "                                  char_to_idx=train_dataset.char_to_idx, build_vocab=False)\n",
    "test_dataset = NERDatasetWithChar(test_file, word_to_idx=train_dataset.word_to_idx,\n",
    "                                   tag_to_idx=train_dataset.tag_to_idx,  # not used in test\n",
    "                                   char_to_idx=train_dataset.char_to_idx, build_vocab=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=pad_collate_with_char)\n",
    "\n",
    "vocab_size = len(train_dataset.word_to_idx)\n",
    "char_vocab_size = len(train_dataset.char_to_idx)\n",
    "num_tags = len(train_dataset.tag_to_idx)\n",
    "    \n",
    "# ----------------------------\n",
    "# Build the LSTM-CNN NER Model\n",
    "# ----------------------------\n",
    "model = LSTM_CNN_NER(vocab_size=vocab_size,\n",
    "                     char_vocab_size=char_vocab_size,\n",
    "                     embedding_dim=100,\n",
    "                     char_embedding_dim=30,\n",
    "                     char_cnn_out_dim=50,\n",
    "                     hidden_dim=256,\n",
    "                     linear_dim=128,\n",
    "                     num_tags=num_tags,\n",
    "                     dropout=0.33)\n",
    "model.to(device)\n",
    "\n",
    "# Initialize word embeddings with GloVe\n",
    "glove_path = \"glove.6B.100d.gz\"\n",
    "glove_weights = load_glove_embeddings(glove_path, train_dataset.word_to_idx, embedding_dim=100)\n",
    "model.word_embedding.weight.data.copy_(glove_weights)\n",
    "\n",
    "# Loss, optimizer (SGD with momentum) and scheduler (ReduceLROnPlateau)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "num_epochs = 10\n",
    "train_model(model, train_loader, optimizer, scheduler, criterion, device, num_epochs)\n",
    "\n",
    "# ----------------------------\n",
    "# Generate predictions on dev data and evaluate using provided eval.py\n",
    "# ----------------------------\n",
    "dev_pred_file = \"predictions_bonus.txt\"\n",
    "generate_prediction_file(model, dev_dataset, device, dev_pred_file)\n",
    "print(\"Calling evaluation script on dev data...\")\n",
    "os.system(f\"python eval.py -p {dev_pred_file} -g {dev_file}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Generate predictions on test data and save to file \"pred\"\n",
    "# ----------------------------\n",
    "test_pred_file = \"pred\"\n",
    "generate_prediction_file(model, test_dataset, device, test_pred_file)\n",
    "print(\"Test predictions saved to file 'pred'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bc1ec0d3-b5df-4c98-9a54-58273ae59153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting additional 5 epochs training...\n",
      "Epoch 1 Loss: 0.0347\n",
      "Epoch 2 Loss: 0.0316\n",
      "Epoch 3 Loss: 0.0279\n",
      "Epoch 4 Loss: 0.0260\n",
      "Epoch 5 Loss: 0.0233\n",
      "Calling evaluation script on dev data...\n",
      "processed 51578 tokens with 5942 phrases; found: 5866 phrases; correct: 4952.\n",
      "accuracy:  97.19%; precision:  84.42%; recall:  83.34%; FB1:  83.88\n",
      "              LOC: precision:  95.19%; recall:  82.91%; FB1:  88.62  1600\n",
      "             MISC: precision:  80.34%; recall:  77.98%; FB1:  79.14  895\n",
      "              ORG: precision:  81.65%; recall:  74.65%; FB1:  77.99  1226\n",
      "              PER: precision:  79.67%; recall:  92.78%; FB1:  85.73  2145\n",
      "Test predictions saved to file 'pred'.\n"
     ]
    }
   ],
   "source": [
    "additional_epochs = 5\n",
    "print(f\"Starting additional {additional_epochs} epochs training...\")\n",
    "train_model(model, train_loader, optimizer, scheduler, criterion, device, additional_epochs)\n",
    "\n",
    "generate_prediction_file(model, dev_dataset, device, dev_pred_file)\n",
    "print(\"Calling evaluation script on dev data...\")\n",
    "os.system(f\"python eval.py -p {dev_pred_file} -g {dev_file}\")\n",
    "\n",
    "test_pred_file = \"pred\"\n",
    "generate_prediction_file(model, test_dataset, device, test_pred_file)\n",
    "print(\"Test predictions saved to file 'pred'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
