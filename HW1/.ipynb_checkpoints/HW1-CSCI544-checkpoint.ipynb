{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/liuming/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/liuming/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/liuming/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/liuming/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/liuming/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import ssl\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import contractions\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in ./.venv/lib/python3.11/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.11/site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.11/site-packages (from beautifulsoup4->bs4) (2.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gj/psnn653s4wsfmbwvblrx02580000gn/T/ipykernel_36612/3121547469.py:3: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path, sep='\\t', usecols=['review_body', 'star_rating'])\n"
     ]
    }
   ],
   "source": [
    "file_path = 'amazon_reviews_us_Office_Products_v1_00.tsv'\n",
    "\n",
    "data = pd.read_csv(file_path, sep='\\t', usecols=['review_body', 'star_rating'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={'review_body': 'Review', 'star_rating': 'Rating'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We form three classes and select 20000 reviews randomly from each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three sample reviews before data cleaning + preprocessing.\n",
      "                                                    Review  Rating\n",
      "602720                                  They hold pencils!     5.0\n",
      "1361624  I have been using these batteries for over a m...     5.0\n",
      "1067633  Nice and Compact - does a perfect job punching...     5.0\n",
      "849640   what a great price! and the cartridges are wor...     5.0\n",
      "1466391  This phone has all the features we were lookin...     4.0\n"
     ]
    }
   ],
   "source": [
    "RANDOM_NUM = 6\n",
    "data['Rating'] = pd.to_numeric(data['Rating'], errors='coerce')\n",
    "\n",
    "#drop NAN Value\n",
    "data = data.dropna(subset=['Rating'])\n",
    "data = data.dropna(subset=['Review'])\n",
    "\n",
    "# Rating > 3 -> 1 positive，Rating <= 2 -> 0 negative，Rating == 3 -> None\n",
    "data['Sentiment'] = data['Rating'].apply(lambda x: 1 if x > 3 else (0 if x <= 2 else None))\n",
    "\n",
    "# drop neutral\n",
    "data = data.dropna(subset=['Sentiment'])\n",
    "\n",
    "# get positive && negative 10,000 comments\n",
    "positive_reviews = data[data['Sentiment'] == 1].sample(10000, random_state=RANDOM_NUM)\n",
    "negative_reviews = data[data['Sentiment'] == 0].sample(10000, random_state=RANDOM_NUM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three sample reviews before data cleaning + preprocessing:\n",
      "                                                    Review  Rating\n",
      "602720                                   they hold pencils     5.0\n",
      "1361624  i have be use these battery for over a month a...     5.0\n",
      "1067633  nice and compact do a perfect job punch all my...     5.0\n",
      "849640   what a great price and the cartridge be work g...     5.0\n",
      "1466391  this phone have all the feature we be look for...     4.0\n",
      "Average review length before cleaning (Positive): 250.43 characters\n",
      "Average review length before cleaning (Negative): 366.44 characters\n",
      "Average review length after cleaning (Positive): 250.43 characters\n",
      "Average review length after cleaning (Negative): 366.44 characters\n"
     ]
    }
   ],
   "source": [
    "# print sample output\n",
    "print(\"Three sample reviews before data cleaning + preprocessing:\")\n",
    "random_reviews = positive_reviews.sample(n=5, random_state=RANDOM_NUM)\n",
    "print(random_reviews[['Review', 'Rating']])\n",
    "\n",
    "# print average len\n",
    "print(f\"Average review length before cleaning (Positive): {positive_reviews['Review'].apply(len).mean():.2f} characters\")\n",
    "print(f\"Average review length before cleaning (Negative): {negative_reviews['Review'].apply(len).mean():.2f} characters\")\n",
    "\n",
    "# 1. lower case\n",
    "positive_reviews['Review'] = positive_reviews['Review'].str.lower()\n",
    "negative_reviews['Review'] = negative_reviews['Review'].str.lower()\n",
    "# 2. remove HTML\n",
    "positive_reviews['Review'] = positive_reviews['Review'].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
    "negative_reviews['Review'] = negative_reviews['Review'].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
    "\n",
    "# 3. remove URL\n",
    "positive_reviews['Review'] = positive_reviews['Review'].apply(lambda x: re.sub(r\"http\\S+|www\\S+\", \"\", x))\n",
    "negative_reviews['Review'] = negative_reviews['Review'].apply(lambda x: re.sub(r\"http\\S+|www\\S+\", \"\", x))\n",
    "\n",
    "# 4. remove non-alphabetical\n",
    "positive_reviews['Review'] = positive_reviews['Review'].apply(lambda x: re.sub(r\"[^a-zA-Z\\s]\", \"\", x))\n",
    "negative_reviews['Review'] = negative_reviews['Review'].apply(lambda x: re.sub(r\"[^a-zA-Z\\s]\", \"\", x))\n",
    "\n",
    "\n",
    "# 5. remove extra spaces\n",
    "positive_reviews['Review'] = positive_reviews['Review'].apply(lambda x: re.sub(r\"\\s+\", \" \", x).strip())\n",
    "negative_reviews['Review'] = negative_reviews['Review'].apply(lambda x: re.sub(r\"\\s+\", \" \", x).strip())\n",
    "\n",
    "\n",
    "# 6. perform contractions\n",
    "positive_reviews['Review'] = positive_reviews['Review'].apply(contractions.fix)\n",
    "negative_reviews['Review'] = negative_reviews['Review'].apply(contractions.fix)\n",
    "\n",
    "\n",
    "# print average len\n",
    "print(f\"Average review length after cleaning (Positive): {positive_reviews['Review'].apply(len).mean():.2f} characters\")\n",
    "print(f\"Average review length after cleaning (Negative): {negative_reviews['Review'].apply(len).mean():.2f} characters\")\n",
    "\n",
    "# random_reviews = positive_reviews.sample(n=5, random_state=RANDOM_NUM)\n",
    "# print(random_reviews[['Review', 'Rating']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Review  Rating\n",
      "602720                                        hold pencils     5.0\n",
      "1361624  using batteries month phones charge like new g...     5.0\n",
      "1067633  nice compact perfect job punching planner inse...     5.0\n",
      "849640       great price cartridges working great also buy     5.0\n",
      "1466391  phone features looking caller id announcement ...     4.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average review length before preprocessing (Positive): {positive_reviews['Review'].apply(len).mean():.2f} characters\")\n",
    "print(f\"Average review length before preprocessing (Negative): {negative_reviews['Review'].apply(len).mean():.2f} characters\")\n",
    "def remove_stop_words(review):\n",
    "    # tokens = word_tokenize(review)\n",
    "    tokens = review.split(\" \")\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "positive_reviews['Review'] = positive_reviews['Review'].apply(remove_stop_words)\n",
    "\n",
    "# random_reviews = positive_reviews.sample(n=5, random_state=RANDOM_NUM)\n",
    "# print(random_reviews[['Review', 'Rating']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average review length after preprocessing (Positive): 263.83 characters\n",
      "Average review length after preprocessing (Negative): 386.62 characters\n",
      "Three sample reviews after data cleaning + preprocessing.\n",
      "                                                    Review  Rating\n",
      "602720                                  They hold pencils!     5.0\n",
      "1361624  I have be use these battery for over a month, ...     5.0\n",
      "1067633  Nice and Compact - do a perfect job punch all ...     5.0\n",
      "849640   what a great price! and the cartridge be work ...     5.0\n",
      "1466391  This phone have all the feature we be look for...     4.0\n"
     ]
    }
   ],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"\n",
    "    将 nltk 的 POS tag 转换为 wordnet 的 pos 参数\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def perform_lemmatization(review):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = review.split(\" \")  # 或者你自己的分词方式\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in pos_tags:\n",
    "        wordnet_pos = get_wordnet_pos(tag)\n",
    "        lemmatized_tokens.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "positive_reviews['Review'] = positive_reviews['Review'].apply(perform_lemmatization)\n",
    "\n",
    "print(f\"Average review length after preprocessing (Positive): {positive_reviews['Review'].apply(len).mean():.2f} characters\")\n",
    "print(f\"Average review length after preprocessing (Negative): {negative_reviews['Review'].apply(len).mean():.2f} characters\")\n",
    "\n",
    "print(\"Three sample reviews after data cleaning + preprocessing:\")\n",
    "random_reviews = positive_reviews.sample(n=5, random_state=RANDOM_NUM)\n",
    "print(random_reviews[['Review', 'Rating']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 190574 stored elements and shape (16000, 500)>\n",
      "  Coords\tValues\n",
      "  (0, 488)\t0.08288234010357325\n",
      "  (0, 319)\t0.0938304819527887\n",
      "  (0, 409)\t0.1597499547953402\n",
      "  (0, 275)\t0.1574999453991393\n",
      "  (0, 273)\t0.26308678909175426\n",
      "  (0, 477)\t0.12801648739656943\n",
      "  (0, 53)\t0.11614456783946249\n",
      "  (0, 179)\t0.3918047596128162\n",
      "  (0, 379)\t0.1657816442731996\n",
      "  (0, 356)\t0.28948054248198646\n",
      "  (0, 189)\t0.16995081720389235\n",
      "  (0, 54)\t0.6300363259105234\n",
      "  (0, 98)\t0.18399078295784368\n",
      "  (0, 449)\t0.1402840052974351\n",
      "  (0, 410)\t0.18531406954435237\n",
      "  (0, 200)\t0.1112099508944366\n",
      "  (0, 103)\t0.11212849386647787\n",
      "  (0, 328)\t0.1774009871267902\n",
      "  (1, 375)\t0.8463911341157393\n",
      "  (1, 200)\t0.5325617786607227\n",
      "  (2, 255)\t0.11248491770100069\n",
      "  (2, 316)\t0.10955218844141115\n",
      "  (2, 237)\t0.11447197559128913\n",
      "  (2, 154)\t0.09458714661581673\n",
      "  (2, 436)\t0.12841668039525425\n",
      "  :\t:\n",
      "  (15999, 27)\t0.26256300052732484\n",
      "  (15999, 356)\t0.09742201993715555\n",
      "  (15999, 50)\t0.21202064483115537\n",
      "  (15999, 0)\t0.1948440398743111\n",
      "  (15999, 384)\t0.12816529134901872\n",
      "  (15999, 486)\t0.22013751931735134\n",
      "  (15999, 250)\t0.081222320599734\n",
      "  (15999, 54)\t0.08481317739385777\n",
      "  (15999, 386)\t0.1168756815050276\n",
      "  (15999, 97)\t0.10596158162877543\n",
      "  (15999, 157)\t0.11355208833055537\n",
      "  (15999, 118)\t0.1271287196311224\n",
      "  (15999, 245)\t0.11321179471649273\n",
      "  (15999, 450)\t0.12246574838462557\n",
      "  (15999, 78)\t0.12495987901669874\n",
      "  (15999, 120)\t0.11541114520951155\n",
      "  (15999, 439)\t0.20288214365858975\n",
      "  (15999, 75)\t0.11209240194985236\n",
      "  (15999, 312)\t0.10752476244236542\n",
      "  (15999, 318)\t0.10591294784707964\n",
      "  (15999, 489)\t0.09261830605588481\n",
      "  (15999, 341)\t0.12675217778374698\n",
      "  (15999, 200)\t0.14970675348823598\n",
      "  (15999, 481)\t0.11072298026450424\n",
      "  (15999, 164)\t0.11222119695604853\n",
      "16530    0.0\n",
      "11201    0.0\n",
      "5247     1.0\n",
      "4603     1.0\n",
      "1312     1.0\n",
      "        ... \n",
      "8527     1.0\n",
      "4714     1.0\n",
      "10196    0.0\n",
      "8419     1.0\n",
      "19145    0.0\n",
      "Name: Sentiment, Length: 16000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 合并正面和负面评论\n",
    "all_reviews = pd.concat([positive_reviews, negative_reviews], ignore_index=True)\n",
    "\n",
    "# 提取评论文本和标签\n",
    "reviews = all_reviews['Review']  # 评论内容\n",
    "labels = all_reviews['Sentiment']    # 二进制标签\n",
    "\n",
    "# 初始化 TF-IDF 向量化器\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500, stop_words='english')  # 保留最多 500 个特征\n",
    "\n",
    "# 提取 TF-IDF 特征\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(reviews)\n",
    "\n",
    "# 转换为 DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# 添加标签到特征集\n",
    "tfidf_df['Sentiment'] = labels.values\n",
    "\n",
    "# 6. 拆分训练集和测试集（80% 训练, 20% 测试）\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_features, labels, test_size=0.2, random_state=RANDOM_NUM)\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9358\n",
      "Training Precision: 0.9351\n",
      "Training Recall: 0.9366\n",
      "Training F1-Score: 0.9359\n",
      "Testing Accuracy: 0.9213\n",
      "Testing Precision: 0.9198\n",
      "Testing Recall: 0.9230\n",
      "Testing F1-Score: 0.9214\n"
     ]
    }
   ],
   "source": [
    "# 初始化并训练感知器模型\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, y_train)\n",
    "\n",
    "# 在训练集上进行预测\n",
    "y_train_pred = perceptron.predict(X_train)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_test_pred = perceptron.predict(X_test)\n",
    "\n",
    "# 计算指标\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred)\n",
    "train_recall = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# 输出结果\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Training Precision: {train_precision:.4f}\")\n",
    "print(f\"Training Recall: {train_recall:.4f}\")\n",
    "print(f\"Training F1-Score: {train_f1:.4f}\")\n",
    "\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Testing Precision: {test_precision:.4f}\")\n",
    "print(f\"Testing Recall: {test_recall:.4f}\")\n",
    "print(f\"Testing F1-Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9496\n",
      "Training Precision: 0.9378\n",
      "Training Recall: 0.9631\n",
      "Training F1-Score: 0.9503\n",
      "Testing Accuracy: 0.9400\n",
      "Testing Precision: 0.9276\n",
      "Testing Recall: 0.9545\n",
      "Testing F1-Score: 0.9409\n"
     ]
    }
   ],
   "source": [
    "# 初始化并训练 SVM 模型\n",
    "svm_model = SVC(kernel='linear', random_state=RANDOM_NUM)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# 在训练集上进行预测\n",
    "y_train_pred = svm_model.predict(X_train)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_test_pred = svm_model.predict(X_test)\n",
    "\n",
    "# 计算训练集指标\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred)\n",
    "train_recall = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "# # 计算测试集指标\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# # 打印结果\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Training Precision: {train_precision:.4f}\")\n",
    "print(f\"Training Recall: {train_recall:.4f}\")\n",
    "print(f\"Training F1-Score: {train_f1:.4f}\")\n",
    "\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Testing Precision: {test_precision:.4f}\")\n",
    "print(f\"Testing Recall: {test_recall:.4f}\")\n",
    "print(f\"Testing F1-Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9472\n",
      "Training Precision: 0.9369\n",
      "Training Recall: 0.9590\n",
      "Training F1-Score: 0.9478\n",
      "Testing Accuracy: 0.9407\n",
      "Testing Precision: 0.9319\n",
      "Testing Recall: 0.9510\n",
      "Testing F1-Score: 0.9414\n"
     ]
    }
   ],
   "source": [
    "logistic_model = LogisticRegression(random_state=RANDOM_NUM)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# 在训练集上进行预测\n",
    "y_train_pred = logistic_model.predict(X_train)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_test_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# 计算训练集指标\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred)\n",
    "train_recall = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "# 计算测试集指标\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# 打印结果\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Training Precision: {train_precision:.4f}\")\n",
    "print(f\"Training Recall: {train_recall:.4f}\")\n",
    "print(f\"Training F1-Score: {train_f1:.4f}\")\n",
    "\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Testing Precision: {test_precision:.4f}\")\n",
    "print(f\"Testing Recall: {test_recall:.4f}\")\n",
    "print(f\"Testing F1-Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9337\n",
      "Training Precision: 0.9285\n",
      "Training Recall: 0.9394\n",
      "Training F1-Score: 0.9339\n",
      "Testing Accuracy: 0.9365\n",
      "Testing Precision: 0.9324\n",
      "Testing Recall: 0.9425\n",
      "Testing F1-Score: 0.9374\n"
     ]
    }
   ],
   "source": [
    "# 使用 TF-IDF 特征提取\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(reviews)\n",
    "\n",
    "# 数据集划分\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 训练 Multinomial Naive Bayes 模型\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# 在训练集和测试集上生成预测\n",
    "y_train_pred = nb_model.predict(X_train)\n",
    "y_test_pred = nb_model.predict(X_test)\n",
    "\n",
    "# 计算评估指标\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred)\n",
    "train_recall = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# 在 Jupyter Notebook 中打印\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Training Precision: {train_precision:.4f}\")\n",
    "print(f\"Training Recall: {train_recall:.4f}\")\n",
    "print(f\"Training F1-Score: {train_f1:.4f}\")\n",
    "\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Testing Precision: {test_precision:.4f}\")\n",
    "print(f\"Testing Recall: {test_recall:.4f}\")\n",
    "print(f\"Testing F1-Score: {test_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
