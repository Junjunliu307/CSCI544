{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/liuming/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/liuming/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/liuming/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/liuming/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/liuming/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import contractions\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in ./.venv/lib/python3.11/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.11/site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.11/site-packages (from beautifulsoup4->bs4) (2.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gj/psnn653s4wsfmbwvblrx02580000gn/T/ipykernel_33449/1955061181.py:4: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path, sep='\\t', usecols=['review_body', 'star_rating'])\n"
     ]
    }
   ],
   "source": [
    "file_path = 'amazon_reviews_us_Office_Products_v1_00.tsv'\n",
    "\n",
    "# 使用 Pandas 读取数据文件\n",
    "data = pd.read_csv(file_path, sep='\\t', usecols=['review_body', 'star_rating'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 仅保留需要的字段，重命名方便后续处理\n",
    "data.rename(columns={'review_body': 'Review', 'star_rating': 'Rating'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We form three classes and select 20000 reviews randomly from each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three sample reviews: \n",
      "                                                    Review  Rating\n",
      "1111215                                      Dreat pencils     5.0\n",
      "2170760  After only 2 weeks of usage my prints were lig...     1.0\n",
      "2318100  All pieces are well crafted with no burrs or s...     5.0\n",
      "\n",
      "Statistics of the ratings:\n",
      "Rating\n",
      "1.0     306980\n",
      "2.0     138388\n",
      "4.0     418358\n",
      "5.0    1582769\n",
      "Name: count, dtype: int64\n",
      "                                                    Review  Rating\n",
      "2520400  This recorder is awesome. Easy to use, lightwe...     5.0\n",
      "97615    I wanted them to write on Ball Plastic lids on...     4.0\n",
      "2355026  I enjoy computers and saving older photos but ...     5.0\n",
      "368600   I don't think these lasted anywhere as long as...     4.0\n",
      "729394   Seems to work perfectly.  No issues with anyth...     5.0\n"
     ]
    }
   ],
   "source": [
    "RANDOM_NUM = 6\n",
    "# 转换 Rating 列为数值类型\n",
    "data['Rating'] = pd.to_numeric(data['Rating'], errors='coerce')\n",
    "\n",
    "# 移除无效评分（NaN 值）\n",
    "data = data.dropna(subset=['Rating'])\n",
    "data = data.dropna(subset=['Review'])\n",
    "\n",
    "# 映射评分为二元情感标签\n",
    "# Rating > 3 映射为 1（正面），Rating <= 2 映射为 0（负面），Rating == 3 丢弃\n",
    "data['Sentiment'] = data['Rating'].apply(lambda x: 1 if x > 3 else (0 if x <= 2 else None))\n",
    "\n",
    "# 丢弃评分为 3 的评论\n",
    "data = data.dropna(subset=['Sentiment'])\n",
    "\n",
    "# 打印示例评论及对应评分\n",
    "sample_reviews = data[['Review', 'Rating']].sample(3)\n",
    "print(\"Three sample reviews: \")\n",
    "print(sample_reviews)\n",
    "\n",
    "# 统计评分的分布\n",
    "rating_counts = data['Rating'].value_counts().sort_index()\n",
    "print(\"\\nStatistics of the ratings:\")\n",
    "print(rating_counts)\n",
    "\n",
    "# 提取正面和负面评论各 100,000 条\n",
    "positive_reviews = data[data['Sentiment'] == 1].sample(100000, random_state=RANDOM_NUM)\n",
    "negative_reviews = data[data['Sentiment'] == 0].sample(100000, random_state=RANDOM_NUM)\n",
    "\n",
    "random_reviews = positive_reviews.sample(n=5, random_state=RANDOM_NUM)\n",
    "print(random_reviews[['Review', 'Rating']])\n",
    "\n",
    "# # 合并正面和负面评论，形成新的数据集\n",
    "# balanced_data = pd.concat([positive_reviews, negative_reviews]).reset_index(drop=True)\n",
    "\n",
    "# # 按 80% 和 20% 的比例分割训练集和测试集\n",
    "# train_data, test_data = train_test_split(\n",
    "#     balanced_data, \n",
    "#     test_size=0.2, \n",
    "#     random_state=42,  # 确保分割的随机性可复现\n",
    "#     stratify=balanced_data['Sentiment']  # 保证正负样本比例一致\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gj/psnn653s4wsfmbwvblrx02580000gn/T/ipykernel_33449/3067440191.py:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  positive_reviews['Review'] = positive_reviews['Review'].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
      "/var/folders/gj/psnn653s4wsfmbwvblrx02580000gn/T/ipykernel_33449/3067440191.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  negative_reviews['Review'] = negative_reviews['Review'].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
      "/var/folders/gj/psnn653s4wsfmbwvblrx02580000gn/T/ipykernel_33449/3067440191.py:6: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  negative_reviews['Review'] = negative_reviews['Review'].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Review  Rating\n",
      "2520400  this recorder is awesome easy to use lightweig...     5.0\n",
      "97615    i wanted them to write on ball plastic lids on...     4.0\n",
      "2355026  i enjoy computers and saving older photos but ...     5.0\n",
      "368600   i do not think these lasted anywhere as long a...     4.0\n",
      "729394   seems to work perfectly no issues with anythin...     5.0\n"
     ]
    }
   ],
   "source": [
    "# 1. 转换为小写\n",
    "positive_reviews['Review'] = positive_reviews['Review'].str.lower()\n",
    "negative_reviews['Review'] = negative_reviews['Review'].str.lower()\n",
    "# 2. 移除 HTML 标签\n",
    "positive_reviews['Review'] = positive_reviews['Review'].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
    "negative_reviews['Review'] = negative_reviews['Review'].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
    "\n",
    "# 3. 移除 URL\n",
    "positive_reviews['Review'] = positive_reviews['Review'].apply(lambda x: re.sub(r\"http\\S+|www\\S+\", \"\", x))\n",
    "negative_reviews['Review'] = negative_reviews['Review'].apply(lambda x: re.sub(r\"http\\S+|www\\S+\", \"\", x))\n",
    "\n",
    "# 4. 移除非字母字符\n",
    "positive_reviews['Review'] = positive_reviews['Review'].apply(lambda x: re.sub(r\"[^a-zA-Z\\s]\", \"\", x))\n",
    "negative_reviews['Review'] = negative_reviews['Review'].apply(lambda x: re.sub(r\"[^a-zA-Z\\s]\", \"\", x))\n",
    "\n",
    "\n",
    "# 5. 删除多余的空格\n",
    "positive_reviews['Review'] = positive_reviews['Review'].apply(lambda x: re.sub(r\"\\s+\", \" \", x).strip())\n",
    "negative_reviews['Review'] = negative_reviews['Review'].apply(lambda x: re.sub(r\"\\s+\", \" \", x).strip())\n",
    "\n",
    "\n",
    "# 6. 展开缩略形式\n",
    "positive_reviews['Review'] = positive_reviews['Review'].apply(contractions.fix)\n",
    "negative_reviews['Review'] = negative_reviews['Review'].apply(contractions.fix)\n",
    "\n",
    "\n",
    "random_reviews = positive_reviews.sample(n=5, random_state=RANDOM_NUM)\n",
    "print(random_reviews[['Review', 'Rating']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Review  Rating\n",
      "2520400  recorder awesome easy use lightweight excellen...     5.0\n",
      "97615    wanted write ball plastic lids jars freezer co...     4.0\n",
      "2355026  enjoy computers saving older photos definitely...     5.0\n",
      "368600                 think lasted anywhere long expected     4.0\n",
      "729394   seems work perfectly issues anything yet great...     5.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# # 数据清洗函数\n",
    "# def preprocess_reviews(data):\n",
    "#     # 初始化停用词和词形还原器\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "#     # 定义处理逻辑\n",
    "#     def clean_and_preprocess(review):\n",
    "#         # 分词\n",
    "#         # tokens = word_tokenize(review)\n",
    "#         tokens = review.split(\" \")\n",
    "#         # 移除停用词并进行词形还原\n",
    "#         processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "#         # 将处理后的词汇重新拼接为字符串\n",
    "#         return ' '.join(processed_tokens)\n",
    "\n",
    "#     # 应用到所有评论\n",
    "#     data['Processed_Review'] = data['Review'].apply(clean_and_preprocess)\n",
    "#     return data\n",
    "# positive_reviews = preprocess_reviews(positive_reviews)\n",
    "# negative_reviews = preprocess_reviews(negative_reviews)\n",
    "\n",
    "def remove_stop_words(review):\n",
    "    # tokens = word_tokenize(review)\n",
    "    tokens = review.split(\" \")\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "positive_reviews['Review'] = positive_reviews['Review'].apply(remove_stop_words)\n",
    "\n",
    "random_reviews = positive_reviews.sample(n=5, random_state=RANDOM_NUM)\n",
    "print(random_reviews[['Review', 'Rating']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Review  Rating\n",
      "2520400  recorder awesome easy use lightweight excellen...     5.0\n",
      "97615    want write ball plastic lid jar freezer conden...     4.0\n",
      "2355026  enjoy computer save old photo definitely techi...     5.0\n",
      "368600                     think last anywhere long expect     4.0\n",
      "729394   seem work perfectly issue anything yet great v...     5.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"\n",
    "    将 nltk 的 POS tag 转换为 wordnet 的 pos 参数\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def perform_lemmatization(review):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = review.split(\" \")  # 或者你自己的分词方式\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in pos_tags:\n",
    "        wordnet_pos = get_wordnet_pos(tag)\n",
    "        lemmatized_tokens.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "positive_reviews['Review'] = positive_reviews['Review'].apply(perform_lemmatization)\n",
    "random_reviews = positive_reviews.sample(n=5, random_state=RANDOM_NUM)\n",
    "print(random_reviews[['Review', 'Rating']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 676090 stored elements and shape (160000, 50)>\n",
      "  Coords\tValues\n",
      "  (1, 45)\t0.43935077743805984\n",
      "  (1, 22)\t0.595353256545023\n",
      "  (1, 47)\t0.6727000775128604\n",
      "  (2, 41)\t0.4313383607056142\n",
      "  (2, 21)\t0.41026131622056333\n",
      "  (2, 15)\t0.3496976140797205\n",
      "  (2, 3)\t0.3359903062638574\n",
      "  (2, 19)\t0.4390103886193848\n",
      "  (2, 30)\t0.4666099468342816\n",
      "  (4, 17)\t1.0\n",
      "  (5, 13)\t0.47575948123928763\n",
      "  (5, 18)\t0.5186106726814753\n",
      "  (5, 30)\t0.710419514225075\n",
      "  (6, 13)\t0.2950980498852048\n",
      "  (6, 45)\t0.2751361274511765\n",
      "  (6, 21)\t0.38743633382995724\n",
      "  (6, 39)\t0.34474595564128685\n",
      "  (6, 22)\t0.3728300890382231\n",
      "  (6, 3)\t0.31729740854068056\n",
      "  (6, 43)\t0.4194464346092762\n",
      "  (6, 31)\t0.3906985197785913\n",
      "  (7, 45)\t0.28784974390081985\n",
      "  (7, 12)\t0.32166734355539633\n",
      "  (7, 44)\t0.6829841871277936\n",
      "  (7, 43)\t0.4388284079625904\n",
      "  :\t:\n",
      "  (159994, 48)\t0.623811812976812\n",
      "  (159994, 21)\t0.45137776209381025\n",
      "  (159994, 38)\t0.4717752565351742\n",
      "  (159994, 24)\t0.42958706358424853\n",
      "  (159995, 48)\t0.2009719381960302\n",
      "  (159995, 26)\t0.3151050632653672\n",
      "  (159995, 10)\t0.2953120322563606\n",
      "  (159995, 40)\t0.5864841950117962\n",
      "  (159995, 23)\t0.5940048897354241\n",
      "  (159995, 17)\t0.27623244331296465\n",
      "  (159996, 48)\t0.14306291449300138\n",
      "  (159996, 34)\t0.3278304852098916\n",
      "  (159996, 13)\t0.3153839709731383\n",
      "  (159996, 33)\t0.37761627765611694\n",
      "  (159996, 45)\t0.4410747095794721\n",
      "  (159996, 24)\t0.3940802406519183\n",
      "  (159996, 39)\t0.18422241100775508\n",
      "  (159996, 32)\t0.19127316354899254\n",
      "  (159996, 14)\t0.45785327109427654\n",
      "  (159997, 48)\t0.6397012422213152\n",
      "  (159997, 18)\t0.7686236534875219\n",
      "  (159998, 34)\t0.5798064639851006\n",
      "  (159998, 28)\t0.8147542355343078\n",
      "  (159999, 3)\t0.6256317024721971\n",
      "  (159999, 23)\t0.7801185633362023\n",
      "116043    0.0\n",
      "89250     1.0\n",
      "75551     1.0\n",
      "101868    0.0\n",
      "148372    0.0\n",
      "         ... \n",
      "195949    0.0\n",
      "4714      1.0\n",
      "41187     1.0\n",
      "117449    0.0\n",
      "162698    0.0\n",
      "Name: Sentiment, Length: 160000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 合并正面和负面评论\n",
    "all_reviews = pd.concat([positive_reviews, negative_reviews], ignore_index=True)\n",
    "\n",
    "# 提取评论文本和标签\n",
    "reviews = all_reviews['Review']  # 评论内容\n",
    "labels = all_reviews['Sentiment']    # 二进制标签\n",
    "\n",
    "# 初始化 TF-IDF 向量化器\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500, stop_words='english')  # 保留最多 500 个特征\n",
    "\n",
    "# 提取 TF-IDF 特征\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(reviews)\n",
    "\n",
    "# 转换为 DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# 添加标签到特征集\n",
    "tfidf_df['Sentiment'] = labels.values\n",
    "\n",
    "# 6. 拆分训练集和测试集（80% 训练, 20% 测试）\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_features, labels, test_size=0.2, random_state=RANDOM_NUM)\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7741\n",
      "Training Precision: 0.7870\n",
      "Training Recall: 0.7521\n",
      "Training F1-Score: 0.7692\n",
      "Testing Accuracy: 0.7722\n",
      "Testing Precision: 0.7836\n",
      "Testing Recall: 0.7505\n",
      "Testing F1-Score: 0.7667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# 初始化并训练感知器模型\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, y_train)\n",
    "\n",
    "# 在训练集上进行预测\n",
    "y_train_pred = perceptron.predict(X_train)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_test_pred = perceptron.predict(X_test)\n",
    "\n",
    "# 计算指标\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred)\n",
    "train_recall = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# 输出结果\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Training Precision: {train_precision:.4f}\")\n",
    "print(f\"Training Recall: {train_recall:.4f}\")\n",
    "print(f\"Training F1-Score: {train_f1:.4f}\")\n",
    "\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Testing Precision: {test_precision:.4f}\")\n",
    "print(f\"Testing Recall: {test_recall:.4f}\")\n",
    "print(f\"Testing F1-Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# 初始化并训练 SVM 模型\n",
    "svm_model = SVC(kernel='linear', random_state=RANDOM_NUM)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# 在训练集上进行预测\n",
    "y_train_pred = svm_model.predict(X_train)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_test_pred = svm_model.predict(X_test)\n",
    "\n",
    "# 计算训练集指标\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred)\n",
    "train_recall = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "# # 计算测试集指标\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# # 打印结果\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Training Precision: {train_precision:.4f}\")\n",
    "print(f\"Training Recall: {train_recall:.4f}\")\n",
    "print(f\"Training F1-Score: {train_f1:.4f}\")\n",
    "\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Testing Precision: {test_precision:.4f}\")\n",
    "print(f\"Testing Recall: {test_recall:.4f}\")\n",
    "print(f\"Testing F1-Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
