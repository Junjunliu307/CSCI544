{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/Homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import ssl\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import contractions\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'amazon_reviews_us_Office_Products_v1_00.tsv'\n",
    "\n",
    "data = pd.read_csv(file_path, sep='\\t', usecols=['review_body', 'star_rating'], low_memory=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={'review_body': 'Review', 'star_rating': 'Rating'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We form three classes and select 20000 reviews randomly from each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of the ratings:\n",
      "Rating\n",
      "5.0    1582769\n",
      "4.0     418358\n",
      "1.0     306980\n",
      "3.0     193683\n",
      "2.0     138388\n",
      "Name: count, dtype: int64\n",
      "Positive Count: 2001127, Negative Count: 445368, Neutral Count: 193683\n"
     ]
    }
   ],
   "source": [
    "RANDOM_NUM = 6\n",
    "data['Rating'] = pd.to_numeric(data['Rating'], errors='coerce')\n",
    "\n",
    "#drop NAN Value\n",
    "data = data.dropna(subset=['Rating'])\n",
    "data = data.dropna(subset=['Review'])\n",
    "\n",
    "print(\"Statistics of the ratings:\")\n",
    "print(data['Rating'].value_counts().sort_values(ascending=False))\n",
    "\n",
    "# Rating > 3 -> 1 positive，Rating <= 2 -> 0 negative，Rating == 3 -> None\n",
    "data['Sentiment'] = data['Rating'].apply(lambda x: 1 if x > 3 else (0 if x <= 2 else None))\n",
    "\n",
    "sentiment_counts = data['Sentiment'].value_counts(dropna=False)\n",
    "print(\"Positive Count: \"+str(sentiment_counts.get(1, 0)) + \", Negative Count: \"+ str(sentiment_counts.get(0, 0)) + \", Neutral Count: \" + str(data['Sentiment'].isna().sum() ))\n",
    "\n",
    "# drop neutral\n",
    "data = data.dropna(subset=['Sentiment'])\n",
    "\n",
    "# get positive && negative 10,000 comments\n",
    "positive_reviews = data[data['Sentiment'] == 1].sample(10000, random_state=RANDOM_NUM)\n",
    "negative_reviews = data[data['Sentiment'] == 0].sample(10000, random_state=RANDOM_NUM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three sample reviews before data cleaning + preprocessing:\n",
      "                                                    Review  Rating\n",
      "602720                                  They hold pencils!     5.0\n",
      "1361624  I have been using these batteries for over a m...     5.0\n",
      "1067633  Nice and Compact - does a perfect job punching...     5.0\n",
      "Average review length before cleaning (Positive): 271.36 characters\n",
      "Average review length before cleaning (Negative): 386.62 characters\n",
      "Average review length after cleaning (Positive): 257.95 characters\n",
      "Average review length after cleaning (Negative): 366.48 characters\n"
     ]
    }
   ],
   "source": [
    "# print sample output\n",
    "print(\"Three sample reviews before data cleaning + preprocessing:\")\n",
    "random_reviews = positive_reviews.sample(n=3, random_state=RANDOM_NUM)\n",
    "print(random_reviews[['Review', 'Rating']])\n",
    "\n",
    "# print average len\n",
    "print(f\"Average review length before cleaning (Positive): {positive_reviews['Review'].apply(len).mean():.2f} characters\")\n",
    "print(f\"Average review length before cleaning (Negative): {negative_reviews['Review'].apply(len).mean():.2f} characters\")\n",
    "\n",
    "# 1. lower case\n",
    "positive_reviews['Review'] = positive_reviews['Review'].str.lower()\n",
    "negative_reviews['Review'] = negative_reviews['Review'].str.lower()\n",
    "# 2. remove HTML\n",
    "positive_reviews['Review'] = positive_reviews['Review'].astype(str).apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text() if \"<\" in x or \">\" in x else x)\n",
    "negative_reviews['Review'] = negative_reviews['Review'].astype(str).apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text() if \"<\" in x or \">\" in x else x)\n",
    "\n",
    "# 3. remove URL\n",
    "positive_reviews['Review'] = positive_reviews['Review'].apply(lambda x: re.sub(r\"http\\S+|www\\S+\", \"\", x))\n",
    "negative_reviews['Review'] = negative_reviews['Review'].apply(lambda x: re.sub(r\"http\\S+|www\\S+\", \"\", x))\n",
    "\n",
    "# 4. remove non-alphabetical\n",
    "positive_reviews['Review'] = positive_reviews['Review'].apply(lambda x: re.sub(r\"[^a-zA-Z\\s]\", \"\", x))\n",
    "negative_reviews['Review'] = negative_reviews['Review'].apply(lambda x: re.sub(r\"[^a-zA-Z\\s]\", \"\", x))\n",
    "\n",
    "\n",
    "# 5. remove extra spaces\n",
    "positive_reviews['Review'] = positive_reviews['Review'].apply(lambda x: re.sub(r\"\\s+\", \" \", x).strip())\n",
    "negative_reviews['Review'] = negative_reviews['Review'].apply(lambda x: re.sub(r\"\\s+\", \" \", x).strip())\n",
    "\n",
    "\n",
    "# 6. perform contractions\n",
    "positive_reviews['Review'] = positive_reviews['Review'].apply(contractions.fix)\n",
    "negative_reviews['Review'] = negative_reviews['Review'].apply(contractions.fix)\n",
    "\n",
    "\n",
    "# print average len\n",
    "print(f\"Average review length after cleaning (Positive): {positive_reviews['Review'].apply(len).mean():.2f} characters\")\n",
    "print(f\"Average review length after cleaning (Negative): {negative_reviews['Review'].apply(len).mean():.2f} characters\")\n",
    "\n",
    "# random_reviews = positive_reviews.sample(n=5, random_state=RANDOM_NUM)\n",
    "# print(random_reviews[['Review', 'Rating']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average review length before preprocessing (Positive): 257.95 characters\n",
      "Average review length before preprocessing (Negative): 366.48 characters\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average review length before preprocessing (Positive): {positive_reviews['Review'].apply(len).mean():.2f} characters\")\n",
    "print(f\"Average review length before preprocessing (Negative): {negative_reviews['Review'].apply(len).mean():.2f} characters\")\n",
    "def remove_stop_words(review):\n",
    "    # tokens = word_tokenize(review)\n",
    "    tokens = review.split(\" \")\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "positive_reviews['Review'] = positive_reviews['Review'].apply(remove_stop_words)\n",
    "\n",
    "# random_reviews = positive_reviews.sample(n=5, random_state=RANDOM_NUM)\n",
    "# print(random_reviews[['Review', 'Rating']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average review length after preprocessing (Positive): 156.88 characters\n",
      "Average review length after preprocessing (Negative): 366.48 characters\n",
      "Three sample reviews after data cleaning + preprocessing:\n",
      "                                                    Review  Rating\n",
      "602720                                         hold pencil     5.0\n",
      "1361624  use battery month phone charge like new great ...     5.0\n",
      "1067633  nice compact perfect job punch planner insert ...     5.0\n"
     ]
    }
   ],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def perform_lemmatization(review):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = review.split(\" \")\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in pos_tags:\n",
    "        wordnet_pos = get_wordnet_pos(tag)\n",
    "        lemmatized_tokens.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "positive_reviews['Review'] = positive_reviews['Review'].apply(perform_lemmatization)\n",
    "\n",
    "print(f\"Average review length after preprocessing (Positive): {positive_reviews['Review'].apply(len).mean():.2f} characters\")\n",
    "print(f\"Average review length after preprocessing (Negative): {negative_reviews['Review'].apply(len).mean():.2f} characters\")\n",
    "\n",
    "print(\"Three sample reviews after data cleaning + preprocessing:\")\n",
    "random_reviews = positive_reviews.sample(n=3, random_state=RANDOM_NUM)\n",
    "print(random_reviews[['Review', 'Rating']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = pd.concat([positive_reviews, negative_reviews], ignore_index=True)\n",
    "\n",
    "reviews = all_reviews['Review']\n",
    "labels = all_reviews['Sentiment']\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500, stop_words='english') \n",
    "\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(reviews)\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "tfidf_df['Sentiment'] = labels.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_features, labels, test_size=0.2, random_state=RANDOM_NUM)\n",
    "\n",
    "# all_reviews = pd.concat([positive_reviews, negative_reviews], ignore_index=True)\n",
    "\n",
    "# reviews = all_reviews['Review']\n",
    "# labels = all_reviews['Sentiment']\n",
    "\n",
    "# X_train_texts, X_test_texts, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=RANDOM_NUM)\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_features=500, stop_words='english')  \n",
    "# X_train = tfidf_vectorizer.fit_transform(X_train_texts)  \n",
    "# X_test = tfidf_vectorizer.transform(X_test_texts)        \n",
    "\n",
    "# tfidf_train_df = pd.DataFrame(X_train.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "# tfidf_train_df['Sentiment'] = y_train.values  \n",
    "# print(X_train)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9330\n",
      "Training Precision: 0.9283\n",
      "Training Recall: 0.9385\n",
      "Training F1-Score: 0.9334\n",
      "Testing Accuracy: 0.9220\n",
      "Testing Precision: 0.9154\n",
      "Testing Recall: 0.9300\n",
      "Testing F1-Score: 0.9226\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = perceptron.predict(X_train)\n",
    "\n",
    "y_test_pred = perceptron.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred)\n",
    "train_recall = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Training Precision: {train_precision:.4f}\")\n",
    "print(f\"Training Recall: {train_recall:.4f}\")\n",
    "print(f\"Training F1-Score: {train_f1:.4f}\")\n",
    "\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Testing Precision: {test_precision:.4f}\")\n",
    "print(f\"Testing Recall: {test_recall:.4f}\")\n",
    "print(f\"Testing F1-Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9496\n",
      "Training Precision: 0.9383\n",
      "Training Recall: 0.9624\n",
      "Training F1-Score: 0.9502\n",
      "Testing Accuracy: 0.9400\n",
      "Testing Precision: 0.9284\n",
      "Testing Recall: 0.9535\n",
      "Testing F1-Score: 0.9408\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC(kernel='linear', random_state=RANDOM_NUM)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = svm_model.predict(X_train)\n",
    "\n",
    "y_test_pred = svm_model.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred)\n",
    "train_recall = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Training Precision: {train_precision:.4f}\")\n",
    "print(f\"Training Recall: {train_recall:.4f}\")\n",
    "print(f\"Training F1-Score: {train_f1:.4f}\")\n",
    "\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Testing Precision: {test_precision:.4f}\")\n",
    "print(f\"Testing Recall: {test_recall:.4f}\")\n",
    "print(f\"Testing F1-Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9463\n",
      "Training Precision: 0.9356\n",
      "Training Recall: 0.9586\n",
      "Training F1-Score: 0.9470\n",
      "Testing Accuracy: 0.9415\n",
      "Testing Precision: 0.9345\n",
      "Testing Recall: 0.9495\n",
      "Testing F1-Score: 0.9420\n"
     ]
    }
   ],
   "source": [
    "logistic_model = LogisticRegression(random_state=RANDOM_NUM)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = logistic_model.predict(X_train)\n",
    "\n",
    "y_test_pred = logistic_model.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred)\n",
    "train_recall = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Training Precision: {train_precision:.4f}\")\n",
    "print(f\"Training Recall: {train_recall:.4f}\")\n",
    "print(f\"Training F1-Score: {train_f1:.4f}\")\n",
    "\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Testing Precision: {test_precision:.4f}\")\n",
    "print(f\"Testing Recall: {test_recall:.4f}\")\n",
    "print(f\"Testing F1-Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9337\n",
      "Training Precision: 0.9285\n",
      "Training Recall: 0.9394\n",
      "Training F1-Score: 0.9339\n",
      "Testing Accuracy: 0.9365\n",
      "Testing Precision: 0.9324\n",
      "Testing Recall: 0.9425\n",
      "Testing F1-Score: 0.9374\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(reviews)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = nb_model.predict(X_train)\n",
    "y_test_pred = nb_model.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred)\n",
    "train_recall = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Training Precision: {train_precision:.4f}\")\n",
    "print(f\"Training Recall: {train_recall:.4f}\")\n",
    "print(f\"Training F1-Score: {train_f1:.4f}\")\n",
    "\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Testing Precision: {test_precision:.4f}\")\n",
    "print(f\"Testing Recall: {test_recall:.4f}\")\n",
    "print(f\"Testing F1-Score: {test_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
