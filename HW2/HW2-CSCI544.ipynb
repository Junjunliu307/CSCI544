{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4aa340c-5bc0-4fad-805a-db43d1d39bb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/liuming/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/liuming/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "dependency:\n",
    "\n",
    "anyascii                  0.3.2\n",
    "anyio                     4.8.0\n",
    "appnope                   0.1.4\n",
    "argon2-cffi               23.1.0\n",
    "argon2-cffi-bindings      21.2.0\n",
    "arrow                     1.3.0\n",
    "asttokens                 3.0.0\n",
    "async-lru                 2.0.4\n",
    "attrs                     25.1.0\n",
    "babel                     2.17.0\n",
    "beautifulsoup4            4.13.1\n",
    "bleach                    6.2.0\n",
    "certifi                   2025.1.33\n",
    "cffi                      1.17.1\n",
    "charset-normalizer        3.4.1\n",
    "click                     8.1.8\n",
    "comm                      0.2.2\n",
    "contractions              0.1.73\n",
    "debugpy                   1.8.12\n",
    "decorator                 5.1.1\n",
    "defusedxml                0.7.1\n",
    "executing                 2.2.0\n",
    "fastjsonschema            2.21.1\n",
    "filelock                  3.17.0\n",
    "fqdn                      1.5.1\n",
    "fsspec                    2025.2.0\n",
    "gensim                    4.3.3\n",
    "h11                       0.14.0\n",
    "httpcore                  1.0.7\n",
    "httpx                     0.28.1\n",
    "idna                      3.10\n",
    "ipykernel                 6.29.5\n",
    "ipython                   8.32.0\n",
    "ipywidgets                8.1.5\n",
    "isoduration               20.11.0\n",
    "jedi                      0.19.2\n",
    "Jinja2                    3.1.5\n",
    "joblib                    1.4.2\n",
    "json5                     0.10.0\n",
    "jsonpointer               3.0.0\n",
    "jsonschema                4.23.0\n",
    "jsonschema-specifications 2024.10.1\n",
    "jupyter                   1.1.1\n",
    "jupyter_client            8.6.3\n",
    "jupyter-console           6.6.3\n",
    "jupyter_core              5.7.2\n",
    "jupyter-events            0.12.0\n",
    "jupyter-lsp               2.2.5\n",
    "jupyter_server            2.15.0\n",
    "jupyter_server_terminals  0.5.3\n",
    "jupyterlab                4.3.5\n",
    "jupyterlab_pygments       0.3.0\n",
    "jupyterlab_server         2.27.3\n",
    "jupyterlab_widgets        3.0.13\n",
    "MarkupSafe                3.0.2\n",
    "matplotlib-inline         0.1.7\n",
    "mistune                   3.1.1\n",
    "mpmath                    1.3.0\n",
    "nbclient                  0.10.2\n",
    "nbconvert                 7.16.6\n",
    "nbformat                  5.10.4\n",
    "nest-asyncio              1.6.0\n",
    "networkx                  3.4.2\n",
    "nltk                      3.9.1\n",
    "notebook                  7.3.2\n",
    "notebook_shim             0.2.4\n",
    "numpy                     1.23.5\n",
    "overrides                 7.7.0\n",
    "packaging                 24.2\n",
    "pandas                    2.2.3\n",
    "pandocfilters             1.5.1\n",
    "parso                     0.8.4\n",
    "pexpect                   4.9.0\n",
    "pillow                    11.1.0\n",
    "pip                       23.1.2\n",
    "platformdirs              4.3.6\n",
    "prometheus_client         0.21.1\n",
    "prompt_toolkit            3.0.50\n",
    "psutil                    6.1.1\n",
    "ptyprocess                0.7.0\n",
    "pure_eval                 0.2.3\n",
    "pyahocorasick             2.1.0\n",
    "pycparser                 2.22\n",
    "Pygments                  2.19.1\n",
    "python-dateutil           2.9.0.post0\n",
    "python-json-logger        3.2.1\n",
    "pytz                      2025.1\n",
    "PyYAML                    6.0.2\n",
    "pyzmq                     26.2.1\n",
    "referencing               0.36.2\n",
    "regex                     2024.11.6\n",
    "requests                  2.32.3\n",
    "rfc3339-validator         0.1.4\n",
    "rfc3986-validator         0.1.1\n",
    "rpds-py                   0.22.3\n",
    "scikit-learn              1.6.1\n",
    "scipy                     1.13.1\n",
    "Send2Trash                1.8.3\n",
    "setuptools                65.5.0\n",
    "six                       1.17.0\n",
    "smart-open                7.1.0\n",
    "sniffio                   1.3.1\n",
    "soupsieve                 2.6\n",
    "stack-data                0.6.3\n",
    "sympy                     1.13.1\n",
    "terminado                 0.18.1\n",
    "textsearch                0.0.24\n",
    "threadpoolctl             3.5.0\n",
    "tinycss2                  1.4.0\n",
    "torch                     2.6.0\n",
    "torchaudio                2.6.0\n",
    "torchvision               0.21.0\n",
    "tornado                   6.4.2\n",
    "tqdm                      4.67.1\n",
    "traitlets                 5.14.3\n",
    "types-python-dateutil     2.9.0.20241206\n",
    "typing_extensions         4.12.2\n",
    "tzdata                    2025.1\n",
    "uri-template              1.3.0\n",
    "urllib3                   2.3.0\n",
    "wcwidth                   0.2.13\n",
    "webcolors                 24.11.1\n",
    "webencodings              0.5.1\n",
    "websocket-client          1.8.0\n",
    "widgetsnbextension        4.0.13\n",
    "wrapt                     1.17.2\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim.downloader as api\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import contractions\n",
    "import ssl\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a3bd23-329e-4b73-8ed8-64419d561ee6",
   "metadata": {},
   "source": [
    "# Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c4dbb05-3585-4e29-8a53-c53adba72057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "file_path = '../data/amazon_reviews_us_Office_Products_v1_00.tsv'\n",
    "data = pd.read_csv(file_path, sep='\\t', usecols=['review_body', 'star_rating'], low_memory=False)\n",
    "\n",
    "data['star_rating'] = pd.to_numeric(data['star_rating'], errors='coerce')\n",
    "data.dropna(subset=['star_rating', 'review_body'], inplace=True)\n",
    "data = data[data['star_rating'].isin([1, 2, 3, 4, 5])]\n",
    "data['sentiment'] = data['star_rating'].apply(lambda x: 1 if x > 3 else (0 if x <= 2 else 2))\n",
    "data.dropna(subset=['sentiment'], inplace=True)\n",
    "\n",
    "balanced_data = pd.concat([\n",
    "    data[data['star_rating'] == 1].sample(n=50000, random_state=6),\n",
    "    data[data['star_rating'] == 2].sample(n=50000, random_state=6),\n",
    "    data[data['star_rating'] == 3].sample(n=50000, random_state=6),\n",
    "    data[data['star_rating'] == 4].sample(n=50000, random_state=6),\n",
    "    data[data['star_rating'] == 5].sample(n=50000, random_state=6)\n",
    "])\n",
    "\n",
    "#pre-processing\n",
    "balanced_data['token'] = balanced_data['review_body'].str.lower()\n",
    "\n",
    "balanced_data['token'] = balanced_data['token'].astype(str).apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text() if \"<\" in x or \">\" in x else x)\n",
    "\n",
    "balanced_data['token'] = balanced_data['token'].apply(lambda x: re.sub(r\"http\\S+|www\\S+\", \"\", x))\n",
    "\n",
    "balanced_data['token'] = balanced_data['token'].apply(lambda x: re.sub(r\"[^a-zA-Z\\s]\", \"\", x))\n",
    "\n",
    "balanced_data['token'] = balanced_data['token'].apply(lambda x: re.sub(r\"\\s+\", \" \", x).strip())\n",
    "\n",
    "balanced_data['token'] = balanced_data['token'].apply(contractions.fix)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    tokens = text.split(\" \")  \n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]  \n",
    "    return tokens\n",
    "\n",
    "balanced_data['token'] = balanced_data['token'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e76cf-81fd-46c2-8da9-75faed090fa5",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7298661e-17ac-475a-b864-75af3fae1598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man + woman = queen 0.7300516963005066\n",
      "Similarity between 'excellent' and 'outstanding': 0.556748628616333\n",
      "king - man + woman = queen 0.1250358670949936\n",
      "Similarity between 'excellent' and 'outstanding': 0.7751966714859009\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "example_words = [\n",
    "    ('king', 'man', 'woman', 'queen'),\n",
    "    ('excellent', 'outstanding')\n",
    "]\n",
    "\n",
    "for words in example_words:\n",
    "    if len(words) == 4:\n",
    "        result = word2vec_model[words[0]] - word2vec_model[words[1]] + word2vec_model[words[2]]\n",
    "        # similar_word = word2vec_model.similar_by_vector(result, topn=1)[0]\n",
    "        similarity = cosine_similarity(result, word2vec_model[words[3]])\n",
    "        print(f\"{words[0]} - {words[1]} + {words[2]} = {words[3]} {similarity}\")\n",
    "    else:\n",
    "        similarity = word2vec_model.similarity(words[0], words[1])\n",
    "        print(f\"Similarity between '{words[0]}' and '{words[1]}': {similarity}\")\n",
    "\n",
    "\n",
    "custom_word2vec = Word2Vec(\n",
    "    sentences=balanced_data['token'],\n",
    "    vector_size=300,  \n",
    "    window=11,        \n",
    "    min_count=10,     \n",
    "    workers=4         \n",
    ")\n",
    "\n",
    "for words in example_words:\n",
    "    if len(words) == 4:\n",
    "        result = custom_word2vec.wv[words[0]] - custom_word2vec.wv[words[1]] + custom_word2vec.wv[words[2]]\n",
    "        similarity = cosine_similarity(result,custom_word2vec.wv[words[3]])\n",
    "        print(f\"{words[0]} - {words[1]} + {words[2]} = {words[3]} {similarity}\")\n",
    "    else:\n",
    "        similarity = custom_word2vec.wv.similarity(words[0], words[1])\n",
    "        print(f\"Similarity between '{words[0]}' and '{words[1]}': {similarity}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f720a8a7-081b-427b-84c6-ad2af546ec35",
   "metadata": {},
   "source": [
    "# Simple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05b66cce-04a9-4ec1-afbe-2b3a7339e2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Accuracy (Google Word2Vec): 0.7630\n",
      "Perceptron Accuracy (Custom Word2Vec): 0.7970\n",
      "Perceptron Accuracy (TF-IDF): 0.4640\n",
      "SVM Accuracy (Google Word2Vec): 0.7880\n",
      "SVM Accuracy (Custom Word2Vec): 0.8490\n",
      "SVM Accuracy (TF-IDF): 0.4670\n"
     ]
    }
   ],
   "source": [
    "def compute_average_word2vec_custom(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "def compute_average_word2vec_google(tokens, model):\n",
    "    vectors = [model[word] for word in tokens if word in model]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(300) \n",
    "\n",
    "balanced_data['w2v_google'] = balanced_data['token'].apply(lambda x: compute_average_word2vec_google(x, word2vec_model))\n",
    "\n",
    "balanced_data['w2v_custom'] = balanced_data['token'].apply(lambda x: compute_average_word2vec_custom(x, custom_word2vec))\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=300)\n",
    "tfidf_features = vectorizer.fit_transform(balanced_data['review_body']).toarray()\n",
    "\n",
    "\n",
    "binary_data = balanced_data[balanced_data['sentiment'].isin([0, 1])]\n",
    "\n",
    "\n",
    "X_train_google, X_test_google, y_train, y_test = train_test_split(\n",
    "    np.vstack(binary_data['w2v_google']), binary_data['sentiment'], test_size=0.2, random_state=6\n",
    ")\n",
    "\n",
    "X_train_custom, X_test_custom, _, _ = train_test_split(\n",
    "    np.vstack(binary_data['w2v_custom']), binary_data['sentiment'], test_size=0.2, random_state=6\n",
    ")\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, _, _ = train_test_split(\n",
    "    tfidf_features[:len(binary_data)], binary_data['sentiment'], test_size=0.2, random_state=6\n",
    ")\n",
    "\n",
    "perceptron_google = Perceptron()\n",
    "perceptron_google.fit(X_train_google, y_train)\n",
    "\n",
    "perceptron_custom = Perceptron()\n",
    "perceptron_custom.fit(X_train_custom, y_train)\n",
    "\n",
    "perceptron_tfidf = Perceptron()\n",
    "perceptron_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_google = perceptron_google.predict(X_test_google)\n",
    "y_pred_custom = perceptron_custom.predict(X_test_custom)\n",
    "y_pred_tfidf = perceptron_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "acc_perceptron_google = accuracy_score(y_test, y_pred_google)\n",
    "acc_perceptron_custom = accuracy_score(y_test, y_pred_custom)\n",
    "acc_perceptron_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "\n",
    "print(f\"Perceptron Accuracy (Google Word2Vec): {acc_perceptron_google:.4f}\")\n",
    "print(f\"Perceptron Accuracy (Custom Word2Vec): {acc_perceptron_custom:.4f}\")\n",
    "print(f\"Perceptron Accuracy (TF-IDF): {acc_perceptron_tfidf:.4f}\")\n",
    "\n",
    "svm_google = SVC(kernel='linear')\n",
    "svm_google.fit(X_train_google, y_train)\n",
    "\n",
    "svm_custom = SVC(kernel='linear')\n",
    "svm_custom.fit(X_train_custom, y_train)\n",
    "\n",
    "svm_tfidf = SVC(kernel='linear')\n",
    "svm_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_svm_google = svm_google.predict(X_test_google)\n",
    "y_pred_svm_custom = svm_custom.predict(X_test_custom)\n",
    "y_pred_svm_tfidf = svm_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "acc_svm_google = accuracy_score(y_test, y_pred_svm_google)\n",
    "acc_svm_custom = accuracy_score(y_test, y_pred_svm_custom)\n",
    "acc_svm_tfidf = accuracy_score(y_test, y_pred_svm_tfidf)\n",
    "\n",
    "print(f\"SVM Accuracy (Google Word2Vec): {acc_svm_google:.4f}\")\n",
    "print(f\"SVM Accuracy (Custom Word2Vec): {acc_svm_custom:.4f}\")\n",
    "print(f\"SVM Accuracy (TF-IDF): {acc_svm_tfidf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf127c94-06c0-4d74-912c-067b4b9b0d23",
   "metadata": {},
   "source": [
    "# Predefined Dataset & Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8abb5c48-7953-43a7-800d-79dd671b6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    '''\n",
    "    self.features = tensor([[0.2314, 0.1234, 0.4321, ..., 0.0456, 0.9874, 0.7890],\n",
    "        [0.0987, 0.2314, 0.2345, ..., 0.2314, 0.4567, 0.2314],\n",
    "        ...\n",
    "       ])\n",
    "    self.labels = tensor([1, 0, 1, 1, 0, 0, 1, ...])\n",
    "    '''\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "def train_model(model, train_loader, test_loader, num_epochs=10, learning_rate=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        train_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "        \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    test_acc = correct / total\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    return model\n",
    "    \n",
    "positive_reviews = balanced_data[balanced_data['sentiment'] == 0]\n",
    "negative_reviews = balanced_data[balanced_data['sentiment'] == 1]\n",
    "neutral_reviews = balanced_data[balanced_data['sentiment'] == 2]\n",
    "\n",
    "all_reviews = pd.concat([positive_reviews, negative_reviews], ignore_index=True)\n",
    "all_reviews_tri = pd.concat([positive_reviews, negative_reviews, neutral_reviews], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f463d-59f5-4873-a10d-9d1eec634db9",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2510f23c-3a0e-4db7-aff4-e63b760ea95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP with Google Word2Vec Mean Features\n",
      "Epoch 1, Loss: 62.5944, Train Accuracy: 0.6986\n",
      "Epoch 2, Loss: 53.3788, Train Accuracy: 0.7852\n",
      "Epoch 3, Loss: 50.7150, Train Accuracy: 0.8081\n",
      "Epoch 4, Loss: 49.4936, Train Accuracy: 0.8177\n",
      "Epoch 5, Loss: 48.7834, Train Accuracy: 0.8223\n",
      "Epoch 6, Loss: 48.1755, Train Accuracy: 0.8300\n",
      "Epoch 7, Loss: 47.5946, Train Accuracy: 0.8356\n",
      "Epoch 8, Loss: 47.2357, Train Accuracy: 0.8394\n",
      "Epoch 9, Loss: 46.7791, Train Accuracy: 0.8431\n",
      "Epoch 10, Loss: 46.4788, Train Accuracy: 0.8442\n",
      "Test Accuracy: 0.8225\n",
      "Training MLP with Custom Word2Vec Mean Features\n",
      "Epoch 1, Loss: 55.6917, Train Accuracy: 0.7534\n",
      "Epoch 2, Loss: 48.3793, Train Accuracy: 0.8263\n",
      "Epoch 3, Loss: 46.5380, Train Accuracy: 0.8477\n",
      "Epoch 4, Loss: 45.3899, Train Accuracy: 0.8561\n",
      "Epoch 5, Loss: 44.5267, Train Accuracy: 0.8658\n",
      "Epoch 6, Loss: 43.7955, Train Accuracy: 0.8784\n",
      "Epoch 7, Loss: 43.1144, Train Accuracy: 0.8833\n",
      "Epoch 8, Loss: 42.7183, Train Accuracy: 0.8878\n",
      "Epoch 9, Loss: 42.1645, Train Accuracy: 0.8992\n",
      "Epoch 10, Loss: 41.7150, Train Accuracy: 0.9014\n",
      "Test Accuracy: 0.8494\n",
      "Training MLP with Google Word2Vec Concatenated Features\n",
      "Epoch 1, Loss: 60.4028, Train Accuracy: 0.6889\n",
      "Epoch 2, Loss: 51.2535, Train Accuracy: 0.7983\n",
      "Epoch 3, Loss: 46.7311, Train Accuracy: 0.8505\n",
      "Epoch 4, Loss: 42.9346, Train Accuracy: 0.8931\n",
      "Epoch 5, Loss: 40.0062, Train Accuracy: 0.9228\n",
      "Epoch 6, Loss: 37.6183, Train Accuracy: 0.9447\n",
      "Epoch 7, Loss: 36.4519, Train Accuracy: 0.9534\n",
      "Epoch 8, Loss: 35.6934, Train Accuracy: 0.9591\n",
      "Epoch 9, Loss: 35.3454, Train Accuracy: 0.9620\n",
      "Epoch 10, Loss: 34.9830, Train Accuracy: 0.9652\n",
      "Test Accuracy: 0.7500\n",
      "Training MLP with Custom Word2Vec Concatenated Features\n",
      "Epoch 1, Loss: 59.6638, Train Accuracy: 0.6964\n",
      "Epoch 2, Loss: 48.4952, Train Accuracy: 0.8248\n",
      "Epoch 3, Loss: 43.3983, Train Accuracy: 0.8822\n",
      "Epoch 4, Loss: 40.1843, Train Accuracy: 0.9161\n",
      "Epoch 5, Loss: 38.0947, Train Accuracy: 0.9375\n",
      "Epoch 6, Loss: 36.9085, Train Accuracy: 0.9481\n",
      "Epoch 7, Loss: 36.2557, Train Accuracy: 0.9541\n",
      "Epoch 8, Loss: 35.7460, Train Accuracy: 0.9578\n",
      "Epoch 9, Loss: 35.2389, Train Accuracy: 0.9628\n",
      "Epoch 10, Loss: 35.0881, Train Accuracy: 0.9644\n",
      "Test Accuracy: 0.7562\n",
      "Training MLP for Ternary Classification (Google Word2Vec Mean)\n",
      "Epoch 1, Loss: 125.6988, Train Accuracy: 0.5707\n",
      "Epoch 2, Loss: 115.6072, Train Accuracy: 0.6259\n",
      "Epoch 3, Loss: 113.2498, Train Accuracy: 0.6399\n",
      "Epoch 4, Loss: 112.0601, Train Accuracy: 0.6498\n",
      "Epoch 5, Loss: 111.3435, Train Accuracy: 0.6575\n",
      "Epoch 6, Loss: 110.5985, Train Accuracy: 0.6597\n",
      "Epoch 7, Loss: 110.0788, Train Accuracy: 0.6637\n",
      "Epoch 8, Loss: 109.1146, Train Accuracy: 0.6710\n",
      "Epoch 9, Loss: 108.4382, Train Accuracy: 0.6754\n",
      "Epoch 10, Loss: 107.9642, Train Accuracy: 0.6805\n",
      "Test Accuracy: 0.6640\n",
      "Training MLP for Ternary Classification (Custom Word2Vec Mean)\n",
      "Epoch 1, Loss: 122.2764, Train Accuracy: 0.5650\n",
      "Epoch 2, Loss: 110.8558, Train Accuracy: 0.6594\n",
      "Epoch 3, Loss: 108.3883, Train Accuracy: 0.6759\n",
      "Epoch 4, Loss: 106.8381, Train Accuracy: 0.6889\n",
      "Epoch 5, Loss: 105.6254, Train Accuracy: 0.7004\n",
      "Epoch 6, Loss: 104.3896, Train Accuracy: 0.7103\n",
      "Epoch 7, Loss: 103.4835, Train Accuracy: 0.7200\n",
      "Epoch 8, Loss: 102.3387, Train Accuracy: 0.7305\n",
      "Epoch 9, Loss: 101.5613, Train Accuracy: 0.7395\n",
      "Epoch 10, Loss: 100.8380, Train Accuracy: 0.7475\n",
      "Test Accuracy: 0.6865\n",
      "Training MLP for Ternary Classification (Google Word2Vec Concatenated)\n",
      "Epoch 1, Loss: 125.5037, Train Accuracy: 0.5380\n",
      "Epoch 2, Loss: 114.6704, Train Accuracy: 0.6286\n",
      "Epoch 3, Loss: 110.2358, Train Accuracy: 0.6700\n",
      "Epoch 4, Loss: 105.8533, Train Accuracy: 0.7009\n",
      "Epoch 5, Loss: 100.5301, Train Accuracy: 0.7585\n",
      "Epoch 6, Loss: 94.8802, Train Accuracy: 0.8061\n",
      "Epoch 7, Loss: 90.4677, Train Accuracy: 0.8425\n",
      "Epoch 8, Loss: 87.1816, Train Accuracy: 0.8669\n",
      "Epoch 9, Loss: 84.9211, Train Accuracy: 0.8812\n",
      "Epoch 10, Loss: 83.1667, Train Accuracy: 0.8928\n",
      "Test Accuracy: 0.5680\n",
      "Training MLP for Ternary Classification (Custom Word2Vec Concatenated)\n",
      "Epoch 1, Loss: 121.3546, Train Accuracy: 0.5664\n",
      "Epoch 2, Loss: 109.8621, Train Accuracy: 0.6645\n",
      "Epoch 3, Loss: 102.9640, Train Accuracy: 0.7261\n",
      "Epoch 4, Loss: 96.6565, Train Accuracy: 0.7811\n",
      "Epoch 5, Loss: 92.5364, Train Accuracy: 0.8159\n",
      "Epoch 6, Loss: 89.1579, Train Accuracy: 0.8438\n",
      "Epoch 7, Loss: 86.9627, Train Accuracy: 0.8611\n",
      "Epoch 8, Loss: 84.9488, Train Accuracy: 0.8756\n",
      "Epoch 9, Loss: 83.8290, Train Accuracy: 0.8844\n",
      "Epoch 10, Loss: 82.8068, Train Accuracy: 0.8928\n",
      "Test Accuracy: 0.5970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentimentClassifier(\n",
       "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        '''\n",
    "        (batch_size,n_featrue) -> (batch_size,128)\n",
    "        (batch_size,128) -> (batch_size,64)\n",
    "        (batch_size,64) -> (batch_size,2)\n",
    "        (batch_size,2) -> (batch_size,2) softmax\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.fc3 = nn.Linear(10, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "        nn.init.kaiming_uniform_(self.fc2.weight)\n",
    "        nn.init.kaiming_uniform_(self.fc3.weight)\n",
    "        \n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "        nn.init.zeros_(self.fc3.bias)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "def compute_average_word2vec_custom(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "def compute_average_word2vec_google(tokens, model):\n",
    "    vectors = [model[word] for word in tokens if word in model]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "\n",
    "all_reviews['w2v_google_mean'] = all_reviews['token'].apply(lambda x: compute_average_word2vec_google(x, word2vec_model))\n",
    "all_reviews['w2v_custom_mean'] = all_reviews['token'].apply(lambda x: compute_average_word2vec_custom(x, custom_word2vec))\n",
    "\n",
    "all_reviews_tri['w2v_google_mean'] = all_reviews_tri['token'].apply(lambda x: compute_average_word2vec_google(x, word2vec_model))\n",
    "all_reviews_tri['w2v_custom_mean'] = all_reviews_tri['token'].apply(lambda x: compute_average_word2vec_custom(x, custom_word2vec))\n",
    "\n",
    "def compute_concatenated_word2vec_custom(tokens, model, max_len=10):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv][:max_len]\n",
    "    while len(vectors) < max_len:\n",
    "        vectors.append(np.zeros(model.vector_size))  \n",
    "    return np.concatenate(vectors)\n",
    "def compute_concatenated_word2vec_google(tokens, model, max_len=10):\n",
    "    vectors = [model[word] for word in tokens if word in model][:max_len]\n",
    "    while len(vectors) < max_len:\n",
    "        vectors.append(np.zeros(model.vector_size))  \n",
    "    return np.concatenate(vectors)\n",
    "\n",
    "all_reviews['w2v_google_concat'] = all_reviews['token'].apply(lambda x: compute_concatenated_word2vec_google(x, word2vec_model))\n",
    "all_reviews['w2v_custom_concat'] = all_reviews['token'].apply(lambda x: compute_concatenated_word2vec_custom(x, custom_word2vec))\n",
    "\n",
    "all_reviews_tri['w2v_google_concat'] = all_reviews_tri['token'].apply(lambda x: compute_concatenated_word2vec_google(x, word2vec_model))\n",
    "all_reviews_tri['w2v_custom_concat'] = all_reviews_tri['token'].apply(lambda x: compute_concatenated_word2vec_custom(x, custom_word2vec))\n",
    "\n",
    "\n",
    "features_google_mean = np.vstack(all_reviews['w2v_google_mean'])\n",
    "features_custom_mean = np.vstack(all_reviews['w2v_custom_mean'])\n",
    "features_google_concat = np.vstack(all_reviews['w2v_google_concat'])\n",
    "features_custom_concat = np.vstack(all_reviews['w2v_custom_concat'])\n",
    "\n",
    "labels = all_reviews['sentiment'].values\n",
    "\n",
    "\n",
    "features_google_mean_tri = np.vstack(all_reviews_tri['w2v_google_mean'])\n",
    "features_custom_mean_tri = np.vstack(all_reviews_tri['w2v_custom_mean'])\n",
    "features_google_concat_tri = np.vstack(all_reviews_tri['w2v_google_concat'])\n",
    "features_custom_concat_tri = np.vstack(all_reviews_tri['w2v_custom_concat'])\n",
    "\n",
    "labels_tri = all_reviews_tri['sentiment'].values\n",
    "\n",
    "X_train_google_mean, X_test_google_mean, y_train, y_test = train_test_split(features_google_mean, labels, test_size=0.2, random_state=6)\n",
    "X_train_custom_mean, X_test_custom_mean, _, _ = train_test_split(features_custom_mean, labels, test_size=0.2, random_state=6)\n",
    "X_train_google_concat, X_test_google_concat, _, _ = train_test_split(features_google_concat, labels, test_size=0.2, random_state=6)\n",
    "X_train_custom_concat, X_test_custom_concat, _, _ = train_test_split(features_custom_concat, labels, test_size=0.2, random_state=6)\n",
    "\n",
    "X_train_google_mean_tri, X_test_google_mean_tri, y_train_tri, y_test_tri = train_test_split(\n",
    "    features_google_mean_tri, labels_tri, test_size=0.2, random_state=6\n",
    ")\n",
    "X_train_custom_mean_tri, X_test_custom_mean_tri, _, _ = train_test_split(\n",
    "    features_custom_mean_tri, labels_tri, test_size=0.2, random_state=6\n",
    ")\n",
    "X_train_google_concat_tri, X_test_google_concat_tri, _, _ = train_test_split(\n",
    "    features_google_concat_tri, labels_tri, test_size=0.2, random_state=6\n",
    ")\n",
    "X_train_custom_concat_tri, X_test_custom_concat_tri, _, _ = train_test_split(\n",
    "    features_custom_concat_tri, labels_tri, test_size=0.2, random_state=6\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset_google_mean = ReviewDataset(X_train_google_mean, y_train)\n",
    "test_dataset_google_mean = ReviewDataset(X_test_google_mean, y_test)\n",
    "\n",
    "train_dataset_custom_mean = ReviewDataset(X_train_custom_mean, y_train)\n",
    "test_dataset_custom_mean = ReviewDataset(X_test_custom_mean, y_test)\n",
    "\n",
    "train_dataset_google_concat = ReviewDataset(X_train_google_concat, y_train)\n",
    "test_dataset_google_concat = ReviewDataset(X_test_google_concat, y_test)\n",
    "\n",
    "train_dataset_custom_concat = ReviewDataset(X_train_custom_concat, y_train)\n",
    "test_dataset_custom_concat = ReviewDataset(X_test_custom_concat, y_test)\n",
    "\n",
    "train_dataset_google_mean_tri = ReviewDataset(X_train_google_mean_tri, y_train_tri)\n",
    "test_dataset_google_mean_tri = ReviewDataset(X_test_google_mean_tri, y_test_tri)\n",
    "\n",
    "train_dataset_custom_mean_tri = ReviewDataset(X_train_custom_mean_tri, y_train_tri)\n",
    "test_dataset_custom_mean_tri = ReviewDataset(X_test_custom_mean_tri, y_test_tri)\n",
    "\n",
    "train_dataset_google_concat_tri = ReviewDataset(X_train_google_concat_tri, y_train_tri)\n",
    "test_dataset_google_concat_tri = ReviewDataset(X_test_google_concat_tri, y_test_tri)\n",
    "\n",
    "train_dataset_custom_concat_tri = ReviewDataset(X_train_custom_concat_tri, y_train_tri)\n",
    "test_dataset_custom_concat_tri = ReviewDataset(X_test_custom_concat_tri, y_test_tri)\n",
    "\n",
    "train_loader_google_mean = DataLoader(train_dataset_google_mean, batch_size=64, shuffle=True)\n",
    "test_loader_google_mean = DataLoader(test_dataset_google_mean, batch_size=64, shuffle=False)\n",
    "\n",
    "train_loader_custom_mean = DataLoader(train_dataset_custom_mean, batch_size=64, shuffle=True)\n",
    "test_loader_custom_mean = DataLoader(test_dataset_custom_mean, batch_size=64, shuffle=False)\n",
    "\n",
    "train_loader_google_concat = DataLoader(train_dataset_google_concat, batch_size=64, shuffle=True)\n",
    "test_loader_google_concat = DataLoader(test_dataset_google_concat, batch_size=64, shuffle=False)\n",
    "\n",
    "train_loader_custom_concat = DataLoader(train_dataset_custom_concat, batch_size=64, shuffle=True)\n",
    "test_loader_custom_concat = DataLoader(test_dataset_custom_concat, batch_size=64, shuffle=False)\n",
    "\n",
    "train_loader_google_mean_tri = DataLoader(train_dataset_google_mean_tri, batch_size=64, shuffle=True)\n",
    "test_loader_google_mean_tri = DataLoader(test_dataset_google_mean_tri, batch_size=64, shuffle=False)\n",
    "\n",
    "train_loader_custom_mean_tri = DataLoader(train_dataset_custom_mean_tri, batch_size=64, shuffle=True)\n",
    "test_loader_custom_mean_tri = DataLoader(test_dataset_custom_mean_tri, batch_size=64, shuffle=False)\n",
    "\n",
    "train_loader_google_concat_tri = DataLoader(train_dataset_google_concat_tri, batch_size=64, shuffle=True)\n",
    "test_loader_google_concat_tri = DataLoader(test_dataset_google_concat_tri, batch_size=64, shuffle=False)\n",
    "\n",
    "train_loader_custom_concat_tri = DataLoader(train_dataset_custom_concat_tri, batch_size=64, shuffle=True)\n",
    "test_loader_custom_concat_tri = DataLoader(test_dataset_custom_concat_tri, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"Training MLP with Google Word2Vec Mean Features\")\n",
    "model_google_mean = SentimentClassifier(input_dim=300, num_classes=2)\n",
    "train_model(model_google_mean, train_loader_google_mean, test_loader_google_mean)\n",
    "\n",
    "print(\"Training MLP with Custom Word2Vec Mean Features\")\n",
    "model_custom_mean = SentimentClassifier(input_dim=300, num_classes=2)\n",
    "train_model(model_custom_mean, train_loader_custom_mean, test_loader_custom_mean)\n",
    "\n",
    "print(\"Training MLP with Google Word2Vec Concatenated Features\")\n",
    "model_google_concat = SentimentClassifier(input_dim=3000, num_classes=2)  # 10 * 300\n",
    "train_model(model_google_concat, train_loader_google_concat, test_loader_google_concat)\n",
    "\n",
    "print(\"Training MLP with Custom Word2Vec Concatenated Features\")\n",
    "model_custom_concat = SentimentClassifier(input_dim=3000, num_classes=2)  # 10 * 300\n",
    "train_model(model_custom_concat, train_loader_custom_concat, test_loader_custom_concat)\n",
    "\n",
    "\n",
    "print(\"Training MLP for Ternary Classification (Google Word2Vec Mean)\")\n",
    "model_google_mean_tri = SentimentClassifier(input_dim=300, num_classes=3)\n",
    "train_model(model_google_mean_tri, train_loader_google_mean_tri, test_loader_google_mean_tri)\n",
    "\n",
    "print(\"Training MLP for Ternary Classification (Custom Word2Vec Mean)\")\n",
    "model_custom_mean_tri = SentimentClassifier(input_dim=300, num_classes=3)\n",
    "train_model(model_custom_mean_tri, train_loader_custom_mean_tri, test_loader_custom_mean_tri)\n",
    "\n",
    "print(\"Training MLP for Ternary Classification (Google Word2Vec Concatenated)\")\n",
    "model_google_concat_tri = SentimentClassifier(input_dim=3000, num_classes=3)  # 10 * 300\n",
    "train_model(model_google_concat_tri, train_loader_google_concat_tri, test_loader_google_concat_tri)\n",
    "\n",
    "print(\"Training MLP for Ternary Classification (Custom Word2Vec Concatenated)\")\n",
    "model_custom_concat_tri = SentimentClassifier(input_dim=3000, num_classes=3)  # 10 * 300\n",
    "train_model(model_custom_concat_tri, train_loader_custom_concat_tri, test_loader_custom_concat_tri)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a596f514-c071-402a-b0b9-857d7dacdce2",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5953aafd-a7b9-4844-a5c1-8e78d919ad7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN with Google Word2Vec\n",
      "Epoch 1, Loss: 62.5583, Train Accuracy: 0.6603\n",
      "Epoch 2, Loss: 51.9975, Train Accuracy: 0.7902\n",
      "Epoch 3, Loss: 49.0883, Train Accuracy: 0.8197\n",
      "Epoch 4, Loss: 46.9726, Train Accuracy: 0.8459\n",
      "Epoch 5, Loss: 45.9028, Train Accuracy: 0.8575\n",
      "Epoch 6, Loss: 44.9589, Train Accuracy: 0.8683\n",
      "Epoch 7, Loss: 43.3237, Train Accuracy: 0.8841\n",
      "Epoch 8, Loss: 42.0923, Train Accuracy: 0.8995\n",
      "Epoch 9, Loss: 40.9464, Train Accuracy: 0.9095\n",
      "Epoch 10, Loss: 39.8343, Train Accuracy: 0.9227\n",
      "Test Accuracy: 0.8313\n",
      "Training CNN with Custom Word2Vec\n",
      "Epoch 1, Loss: 57.6043, Train Accuracy: 0.7325\n",
      "Epoch 2, Loss: 47.6273, Train Accuracy: 0.8327\n",
      "Epoch 3, Loss: 44.3549, Train Accuracy: 0.8683\n",
      "Epoch 4, Loss: 42.4263, Train Accuracy: 0.8908\n",
      "Epoch 5, Loss: 40.8278, Train Accuracy: 0.9084\n",
      "Epoch 6, Loss: 39.2104, Train Accuracy: 0.9273\n",
      "Epoch 7, Loss: 38.4052, Train Accuracy: 0.9347\n",
      "Epoch 8, Loss: 37.6594, Train Accuracy: 0.9409\n",
      "Epoch 9, Loss: 37.2303, Train Accuracy: 0.9437\n",
      "Epoch 10, Loss: 36.6975, Train Accuracy: 0.9497\n",
      "Test Accuracy: 0.8300\n",
      "Training CNN for Ternary Classification (Google Word2Vec)\n",
      "Epoch 1, Loss: 127.6758, Train Accuracy: 0.5055\n",
      "Epoch 2, Loss: 115.2892, Train Accuracy: 0.6241\n",
      "Epoch 3, Loss: 112.1173, Train Accuracy: 0.6500\n",
      "Epoch 4, Loss: 110.3857, Train Accuracy: 0.6644\n",
      "Epoch 5, Loss: 108.4206, Train Accuracy: 0.6837\n",
      "Epoch 6, Loss: 107.2409, Train Accuracy: 0.6950\n",
      "Epoch 7, Loss: 105.8498, Train Accuracy: 0.7064\n",
      "Epoch 8, Loss: 104.8725, Train Accuracy: 0.7146\n",
      "Epoch 9, Loss: 103.5328, Train Accuracy: 0.7256\n",
      "Epoch 10, Loss: 102.4669, Train Accuracy: 0.7341\n",
      "Test Accuracy: 0.6745\n",
      "Training CNN for Ternary Classification (Custom Word2Vec)\n",
      "Epoch 1, Loss: 121.0463, Train Accuracy: 0.5716\n",
      "Epoch 2, Loss: 110.4554, Train Accuracy: 0.6620\n",
      "Epoch 3, Loss: 107.7081, Train Accuracy: 0.6861\n",
      "Epoch 4, Loss: 105.2346, Train Accuracy: 0.7090\n",
      "Epoch 5, Loss: 103.6540, Train Accuracy: 0.7224\n",
      "Epoch 6, Loss: 101.9827, Train Accuracy: 0.7380\n",
      "Epoch 7, Loss: 100.8175, Train Accuracy: 0.7462\n",
      "Epoch 8, Loss: 100.2084, Train Accuracy: 0.7508\n",
      "Epoch 9, Loss: 99.2150, Train Accuracy: 0.7562\n",
      "Epoch 10, Loss: 97.8637, Train Accuracy: 0.7709\n",
      "Test Accuracy: 0.6685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentimentCNN(\n",
       "  (conv1): Conv1d(300, 50, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (conv2): Conv1d(50, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (relu): ReLU()\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc): Linear(in_features=120, out_features=3, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 50 \n",
    "class SentimentCNN(nn.Module):\n",
    "    def __init__(self, input_dim=300, num_classes=2):\n",
    "        super(SentimentCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=50, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=50, out_channels=10, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.flattened_size = self._get_conv_output((1, input_dim, MAX_LEN))\n",
    "        \n",
    "        self.fc = nn.Linear(self.flattened_size, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(shape)  \n",
    "            x = self.conv1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.pool(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.pool(x)\n",
    "            return x.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1) \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "\n",
    "def compute_cnn_word2vec_custom(tokens, model, max_len=MAX_LEN):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv][:max_len]\n",
    "    while len(vectors) < max_len:\n",
    "        vectors.append(np.zeros(model.vector_size))\n",
    "    return np.array(vectors)\n",
    "def compute_cnn_word2vec_google(tokens, model, max_len=MAX_LEN):\n",
    "    vectors = [model[word] for word in tokens if word in model][:max_len]\n",
    "    while len(vectors) < max_len:\n",
    "        vectors.append(np.zeros(model.vector_size))\n",
    "    return np.array(vectors)\n",
    "\n",
    "all_reviews['w2v_google_cnn'] = all_reviews['token'].apply(lambda x: compute_cnn_word2vec_google(x, word2vec_model))\n",
    "all_reviews['w2v_custom_cnn'] = all_reviews['token'].apply(lambda x: compute_cnn_word2vec_custom(x, custom_word2vec))\n",
    "\n",
    "all_reviews_tri['w2v_google_cnn'] = all_reviews_tri['token'].apply(lambda x: compute_cnn_word2vec_google(x, word2vec_model))\n",
    "all_reviews_tri['w2v_custom_cnn'] = all_reviews_tri['token'].apply(lambda x: compute_cnn_word2vec_custom(x, custom_word2vec))\n",
    "\n",
    "\n",
    "features_google_cnn = np.stack(all_reviews['w2v_google_cnn'])\n",
    "features_custom_cnn = np.stack(all_reviews['w2v_custom_cnn'])\n",
    "\n",
    "features_google_cnn_tri = np.stack(all_reviews_tri['w2v_google_cnn'])\n",
    "features_custom_cnn_tri = np.stack(all_reviews_tri['w2v_custom_cnn'])\n",
    "\n",
    "labels = all_reviews['sentiment'].values \n",
    "labels_tri = all_reviews_tri['sentiment'].values \n",
    "\n",
    "X_train_google_cnn, X_test_google_cnn, y_train, y_test = train_test_split(\n",
    "    features_google_cnn, labels, test_size=0.2, random_state=6\n",
    ")\n",
    "X_train_custom_cnn, X_test_custom_cnn, _, _ = train_test_split(\n",
    "    features_custom_cnn, labels, test_size=0.2, random_state=6\n",
    ")\n",
    "\n",
    "X_train_google_cnn_tri, X_test_google_cnn_tri, y_train_tri, y_test_tri = train_test_split(\n",
    "    features_google_cnn_tri, labels_tri, test_size=0.2, random_state=6\n",
    ")\n",
    "X_train_custom_cnn_tri, X_test_custom_cnn_tri, _, _ = train_test_split(\n",
    "    features_custom_cnn_tri, labels_tri, test_size=0.2, random_state=6\n",
    ")\n",
    "\n",
    "train_dataset_google_cnn = ReviewDataset(X_train_google_cnn, y_train)\n",
    "test_dataset_google_cnn = ReviewDataset(X_test_google_cnn, y_test)\n",
    "train_dataset_custom_cnn = ReviewDataset(X_train_custom_cnn, y_train)\n",
    "test_dataset_custom_cnn = ReviewDataset(X_test_custom_cnn, y_test)\n",
    "\n",
    "\n",
    "train_dataset_google_cnn_tri = ReviewDataset(X_train_google_cnn_tri, y_train_tri)\n",
    "test_dataset_google_cnn_tri = ReviewDataset(X_test_google_cnn_tri, y_test_tri)\n",
    "train_dataset_custom_cnn_tri = ReviewDataset(X_train_custom_cnn_tri, y_train_tri)\n",
    "test_dataset_custom_cnn_tri = ReviewDataset(X_test_custom_cnn_tri, y_test_tri)\n",
    "\n",
    "train_loader_google_cnn = DataLoader(train_dataset_google_cnn, batch_size=64, shuffle=True)\n",
    "test_loader_google_cnn = DataLoader(test_dataset_google_cnn, batch_size=64, shuffle=False)\n",
    "train_loader_custom_cnn = DataLoader(train_dataset_custom_cnn, batch_size=64, shuffle=True)\n",
    "test_loader_custom_cnn = DataLoader(test_dataset_custom_cnn, batch_size=64, shuffle=False)\n",
    "\n",
    "train_loader_google_cnn_tri = DataLoader(train_dataset_google_cnn_tri, batch_size=64, shuffle=True)\n",
    "test_loader_google_cnn_tri = DataLoader(test_dataset_google_cnn_tri, batch_size=64, shuffle=False)\n",
    "train_loader_custom_cnn_tri = DataLoader(train_dataset_custom_cnn_tri, batch_size=64, shuffle=True)\n",
    "test_loader_custom_cnn_tri = DataLoader(test_dataset_custom_cnn_tri, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"Training CNN with Google Word2Vec\")\n",
    "model_google_cnn = SentimentCNN(input_dim=300, num_classes=2)  \n",
    "train_model(model_google_cnn, train_loader_google_cnn, test_loader_google_cnn)\n",
    "\n",
    "print(\"Training CNN with Custom Word2Vec\")\n",
    "model_custom_cnn = SentimentCNN(input_dim=300, num_classes=2)  \n",
    "train_model(model_custom_cnn, train_loader_custom_cnn, test_loader_custom_cnn)\n",
    "\n",
    "print(\"Training CNN for Ternary Classification (Google Word2Vec)\")\n",
    "model_google_cnn_tri = SentimentCNN(input_dim=300, num_classes=3) \n",
    "train_model(model_google_cnn_tri, train_loader_google_cnn_tri, test_loader_google_cnn_tri)\n",
    "\n",
    "print(\"Training CNN for Ternary Classification (Custom Word2Vec)\")\n",
    "model_custom_cnn_tri = SentimentCNN(input_dim=300, num_classes=3) \n",
    "train_model(model_custom_cnn_tri, train_loader_custom_cnn_tri, test_loader_custom_cnn_tri)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
